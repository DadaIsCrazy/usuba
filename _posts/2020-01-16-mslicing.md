---
layout: post
title: mslicing
date: "2020-01-16 00:00:00"
description: mslicing presentation
lang: en
locale: en_US
author: Darius Mercadier
excerpt: The previous post introduced bitslicing, a technique to speed up cipher implementations by introducing huge amounts of data-parallelism. However, this technique cannot be applied on ciphers relying on arithmetic operations, puts a lot of pressure on registers, and requires a lot of independent inputs to be efficient. In order to overcome those issues, we propose a generalization of bitslicing called mslicing. Msliced codes use much fewer registers than bitsliced ones, and can fully exploit the capabilities of SIMD extensions.
comments: false
hidden: true
---


As shown in the [previous post]({{ site.baseurl }}{% post_url
2020-01-14-bitslicing %}), bitslicing can produce very
high-throughputs cipher implementations through data-parallelism, but
has a few limitations:

 - it requires _a lot_ of independent inputs to be efficient, since
   the parallelism comes from different inputs.
   
 - it puts a lot of pressure on the registers, which causes spilling
   (moving data from memory to registers and back), thus reducing the
   performances.

 - it cannot efficiently implement ciphers which rely on arithmetic
   operations, like Chacha20 [1] and Threefish [2].


To overcome the first two issues, Kasper & Schwabe [3] proposed a
"byte-sliced" implementation of AES: instead of representing the
128-bit block as 1 bit in 128 registers as bitslicing would, they
represent it as 16 bits in 8 registers. Using only 8 registers greatly
reduces register pressure and allows AES to be implemented without any
spilling. Furthermore, this representation requires less parallel
inputs to fully fill the registers and thus maximize throughput: on
AES, only 8 parallel inputs are required to fill up 128-bit SSE
registers (versus 128 with bitslicing). 


Depending on how excatly data will be put in the registers, some
operations will be available and other will not. In the case of an
AES-like input, which becomes 16 bits in 8 registers, we have two
choices: either put the bits contiguously in the registers, or split
them within the registers. The first option, which we call _vslicing_,
will enable to use of packed instructions (_e.g._ 16-bit addition),
while the second option, which we call _hslicing_, while enable the
use of permutations instructions (_e.g._ `pshufb`). This post will
explain in detail how each technique works, and how they can be used
to improve on bitslicing.


<!--
Vertical slicing
 - =~ vectorization
 - bits are packed together
   * example: Rectangle (re-use slide from PLDI)
 - can use arithmetic operations
 - best implems of Serpent and Chacha use Vslicing
-->
### Vertical slicing


Vertical slicing (or vslicing for short) is similar to the notion of
[_vectorization_](https://en.wikipedia.org/wiki/Automatic_vectorization)
in the compilation world. The idea of vslicing is to use SIMD the way
they were initially intended to: to compute several times the same
operation on different data. For instance, to compute 8 additions in
parallel on 64-bit MMX registers, one can use the `paddb` instruction:

<p align="center">
<img src="{{ site.baseurl }}/assets/images/blog/paddb-small.png">
</p>

Compilers typically use this trick to optimize loops. However, in the
case of vslicing, this is a whole-program transformation; quite like
vectorizing an entire cipher.

For instance, the Rectangle cipher [4] (presented [earlier]({{
site.baseurl }}{% post_url 2020-01-07-usuba-genesis %})) uses a 64-bit
block. We can split it into 4 16-bit elements, and arrange them in the
first 16-bit packed elements of an SSE register:

<p align="center">
<img src="{{ site.baseurl }}/assets/images/blog/vslicing_oneway_small.png">
</p>

Then, applying the same principle as bitslicing, we can fill the
remaining empty elements of those SSE registers with independent
inputs. The second input would go in the second packed elements of
each registers:

<p align="center">
<img src="{{ site.baseurl }}/assets/images/blog/vslicing_twoway_small.png">
</p>

And so on until the registers are full (in the case of Rectangle on
128-bit SSE registers, this is achieved with 8 inputs).

Then, rather than using the bit as the atomic unit of computation, we
can use 16-bit words in the case of Rectangle, or more generally any
size _m_ supported by the architecture (typically _m_ is 8, 16, 32 or
64). As such, _vertical_ _m_-bit SIMD instructions can be used, that
is, instructions which process several vectors element-wise. Such
instructions include additions, multiplications, shifts and
rotations. Hence, Vslicing can be used to implement arithmetic-based,
thus fixing one of the main weaknesses of bitslicing.

However, vslicing is weaker than bitslicing when it comes to
permutations. Applying an arbitrary bit-permutation to the whole block
needs to be done manually using shifts and masks to extracts bits from
the registers and recombine them. By comparison, this could be done at
compile time with bitslicing. Nevertheless, for some ciphers, like
AES, where permutations are only done within each 16-bit elements (as
opposed to "between each 16-bit elements as well"), a better layout of
the data can be used to perform more efficient permutations. This
layout is the basis of horizontal slicing.


<!--
Horizontal slicing
 - cf example from previous paragraph
 - bits are splitted within the registers
 - still cannot use arithmetic
 - can use permutations
   + example? (eg, vpshufb on AES's shiftrows?)
 - best implementations of AES use hslicing
-->
### Horizontal slicing

Rather than considering an _m_-bit atom as a single packed element, we
may also dispatch its _m_ bits into _m_ distinct packed elements,
assuming that m is less or equal to the number of packed elements of
the architecture. On Rectangle, the 64-bit input is again seen as 4
times 16 bits, but this time, each bit goes into a distinct packed
element:

<p align="center">
<img src="{{ site.baseurl }}/assets/images/blog/hslicing_oneway_small.png">
</p>

Once again, there are unused bits in the registers, which can be
filled with subsequent inputs. The second input would go into the
second bits of each packed elements:

<p align="center">
<img src="{{ site.baseurl }}/assets/images/blog/hslicing_twoway_small.png">
</p>

Like for vslicing, this is repeated until the registers are full,
which in the case of Rectangle on SSE requires a total of 8 inputs.


Using this representation, we lose the ability to perform arithmetic
operations but gain the ability to arbitrarily shuffle bits of our
_m_-bit word in a single instruction (for instance using `pshufb` on
SSE registers). This style relies on horizontal SIMD operations: we
can perform element-wise computations within a single register, _i.e._
along an horizontal direction. We call this technique _horizontal_
slicing, or hslicing for short. 

If a cipher can be implemented in such a way that no inter-register
permutations are needed, then hslicing is often more efficient than
bitslicing, mainly because it considerably lowers the register
pressure. Intra-register permutations are slighly slower than with
bitslicing (since they are free in bitslicing), but are still
efficient as they can be performed using a single shuffle instruction.


**Terminology**. Whenever the slicing direction (_i.e._ vertical or
horizontal) is unimportant, we talk about _m-slicing_ and we call
_slicing_ the technique encompassing bitslicing and _m_-slicing.


### Different slicings for different circumstances

The following picture summarizes all slicing types:


<p align="center">
<img src="{{ site.baseurl }}/assets/images/blog/slicings_small.png">
</p>

All those slicing types share the same basic properties as bitslicing:
they are constant-time, they rely on data-parallelism to provide high
throughputs, and they use some sort of tranposition to convert the
data into a format suitable to their model.

However, each slicing type has its own strenghts and weaknesses:

 - transposing data to hslicing or vslicing usually has a cost almost
   negligible compared to a full cipher, while a bitslice
   transposition is much more expensive.
   
 - bitslicing always introduces a lot of spilling, thus reducing its
   potential, unlike hslicing and vslicing.
   
 - only vslicing is a viable option on ciphers using arithmetic
   operations, since it can use CPU SIMD arithmetic instructions. As
   shown in the [previous post]({{ site.baseurl }}{% post_url
   2020-01-14-bitslicing %}), trying implement those instructions in
   bitslicing (and hslicing) does not yield good performances.
   
 - bitslicing is much better at doing permutations than both hslicing
   and vslicing, since it can do any [permutation at compile-time]({{
   site.baseurl }}{% post_url 2020-01-14-bitslicing
   %}#compile-time-permutations). Intra-register permutations can
   still be done efficiently (but at run time) in hslicing using SIMD
   shuffle instructions. Vslicing will only be a reasonable option in
   the absence of permutations, or when permutations can be easily
   expressed using rotations or shifts (like Rectangle's permutation
   for instance, which can be written as 3 rotations as shown in the
   [introduction]({{ site.baseurl }}{% post_url
   2020-01-07-usuba-genesis %})).
   
 - bitslicing requires much more data to provide a high-throughput
   than hslicing and vslicing. On Rectangle for instance, 8
   independent inputs are required to maximize the throughput on SSE
   registers using <i>m</i>slicing, while 128 are required when using
   bitslicing.

 - both vslicing and hslicing rely on some vector instructions, and
   are thus only available on CPUs with SIMD extensions, while
   bitslicing doesn't require any hardware support beside the ability
   to perform bitwise operations on registers.


The choice between bitslicing, vslicing and hslicing thus depends on
both the cipher and the target architecture. For instance, on CPUs
with SIMD extensions, the fastest implementations of DES are
bitsliced, the fastest implementations of Chacha20 are vsliced, while
the fastest AES implementations are hsliced. Even for a given cipher,
some slicings might be faster depending on the architecture: on
X86-64, the fastest implementation of Rectangle is bitsliced, while on
AVX, vslicing is ahead of both bitslicing and vslicing.



---

## References

[1] D. J. Bernstein, [Chacha, a variant of Salsa20](https://cr.yp.to/chacha/chacha-20080128.pdf), 2008.

[2] N. Ferguson _et al._, [The Skein Hash Function Family](https://www.schneier.com/academic/paperfiles/skein1.3.pdf), 2010.

[3] E. Käsper, P. Schwabe, [Faster and Timing-Attack Resistant AES-GCM](https://www.esat.kuleuven.be/cosic/publications/article-1261.pdf), CHES, 2009.

[4] W. Zhang _et al_, [RECTANGLE: A Bit-slice Lightweight Block Cipher Suitable for Multiple Platforms](https://eprint.iacr.org/2014/084.pdf), 2014.
