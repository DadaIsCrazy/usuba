---
layout: post
title: Bitslice adder
date: "2020-01-28 00:00:00"
description: Comparaison of native addition and a bitslice adder
lang: en
locale: en_US
author: Darius Mercadier
excerpt: Bitslicing is supposedly slow when it comes to arithmetic operations, since bitsliced program need to manually reimplement arithmetic using logical gates. In this post, we will show how much of a slowdown this would incur on an Intel Skylake CPU.
comments: false
hidden: true
---

<!--
Introduction
 - bitslicing cannot do arithmetic
   -> must re-implement arithmetic
 - example: adder
   + 5n instructions
   + 5n/m instr/add
   + cost not obvious: superscalar, register pressure...
-->

As explained in post [2 - Bitslicing]({{ site.baseurl }}{% post_url
2020-01-14-bitslicing %}), bitsliced codes cannot use arithmetic
instructions. Instead, arithmetic must be reimplemented using bitwise
logical gates. While this certainly increases code size compared to a
non-bitsliced code, it's not obvious how it affects performances: on
one hand, a simple arithmetic operation becomes a large circuit, but
on the other hand, thanks to bitslicing, this circuit computes _m_
times the operation on _m_-bit registers.

In this post, we will re-use the example of the adder (cf. post
[Bitslicing - Arithmetic operations]({{ site.baseurl }}{% post_url
2020-01-14-bitslicing %}#arithmetic-operations)) to evaluate what
performances we might expect from bitsliced arithmetic. Recall that a
_n_-bit ripple-carry adder is a chain of _n_ full adders, each
composed of 5 instructions, for a total of _n*5_ instructions. Running
this bitsliced adder on _m_-bit registers computes _m_ additions in
parallel, thus reducing its cost to _n*5/m_ instructions per addition.

However, this adder is unlikely to execute in exactly _n*5_ cycles on
modern high-end CPUs, whose microarchitectures are complex: number of
registers, superscalarity, and cache latency will all impact
performances. In this post, we will benchmark such an adder against
native `add` instructions, and show that in most cases, bitsliced
additions are less efficient that native ones.

## Sylake CPU background

<!--
Skylake CPU background
 - not presenting everything
 - schema?
 - pipelined
 - superscalar
 - frontend (in order)
   + decoding (example: inc/jmp)
   + DSB
   + LSD
   + macro-fusion
 - backend (out of order; retiring in order)
   + execution ports:
     * schema?
     * p0156 int ALU (p06 branch) (p015 vect ALU)
     * p23 load
     * p4 store
   + scheduling -> black box
 - memory
   + cache: line size, latency
-->

## Setup

<!-- Setup
  - loop X time over 1 addition
  - bitslice add
    + 8, 16, 32 bits: register pressure will matter at > 16
  - native add:
    + sse: 4x32, 8x16, 16x8
    + GP: 1x32, 1x16, 1x8
  - native add "parallel":
    + because superscalar: saturate p0156
  - Skylake i5-6500
-->

We consider 3 different additions for our evaluation:

 - a bitslice ripple-carry adder (presented above). Three variants are
   used: 8-bit, 16-bit and 32-bit. Since their number of instructions
   are _5*n_, we could expect the 32-bit adder to be twice slower than
   the 16-bit, which itself would be twice slower than the 8-bit
   adder. However, larger adders put more pressure on registers, which
   could reduce performances, hence the need for benchmarking all
   three versions.
   
 - an addition done using a single CPU instruction. For completness,
   we consider the 8, 16 and 32-bit variants of the addition; all of
   which should have the same throughput and latency. On general
   purpose registers, only one addition can be done with a single
   instruction. However, SSE instructions allow to do 4 32-bit, or 8
   16-bit or 16 8-bit additions with a single instruction. This does
   not reduce latency, but increases throughput.
 
 - 3 independent additions done with 3 CPU instructions. Doing only a
   single addition per cycle under-utilizes the superscalar
   capabilities of modern CPUs: up to 3 SSE additions (and 4 on
   general purpose registers) can be done each cycles. Once again, we
   consider the 8, 16 and 32-bit variants, on both SSE and general
   purpose registers.

We implemented those additions in C, and put each of them in a loop
doing 2 billion iterations. This takes less than a second for the
second and third additions, which might be considered too short for
some benchmarks, but since we are merely micro-benchmarking a handful
of CPU instructions, this is enough. We repeat this process 10 times,
and report the average of the 10 runs in the next section. The
standard deviation being less than 5 percents, we omit it in order to
avoid needlessly obsucating the result table.

We compiled our C codes using Clang 7.0.0, but given the simplicity of
the generated assembly codes, the choice of compiler should not
matter. Finally, we ran the benchmarks on a Intel Skylake i5-6500.


## Results

<!--
 - native_single: 
   + expected: 1 add + 1 increment + 1 jump => 1 cycle
   + got: 1 cycle
 - native_parallel:
   + expected: 3 add + 1 increment + 1 jump => 1 cycle
     * macro-fusion
   + got: 1 cycle
 - bitslice:
   + total ops: 5*n. 
   + expected: assuming 3 ops/cycle -> 5n/3
     * 8-bit: exp 8*5/3=13.3, got: 14.77, close enough!
     * 16-bit: exp 16*5/3=26.5, got: 33.28
       & 32-bit: exp 32*5/3=53.3, got: 74.55
       - spilling: half the instructions are `move`
 - bitslice vs others?
-->

We report in the following table the cycles per iteration and cycles
per addition. `bitslice` is the bitsliced addition using a
ripple-carry adder. `native_single` is a single native additions using
a single instruction. `native_parallel` is the three independent
native additions.

<center>

<table style="border-collapse: collapse; text-align:center">
<tr style="border-top:1px solid #c6cbd1;border-right:1px solid #c6cbd1;border-left:1px solid #c6cbd1;background-color:#ffffff">
<th rowspan="2" style="min-width:250px">addition type</th><th style="min-width:200px">cycles/iteration</th><th style="min-width:200px">cycles/add</th></tr>
<tr style="border-bottom:1px solid #c6cbd1;border-right:1px solid #c6cbd1;border-left:1px solid #c6cbd1;background-color:#ffffff"><th>(SSE / GP)</th><th>(SSE / GP)</th></tr>


<tr style="border:1px solid #c6cbd1;background-color:#e6f7ff"><td style="border:1px solid #c6cbd1">  8-bit        bitslice </td><td style="border:1px solid #c6cbd1">     14.77    /    16.28     </td><td style="border:1px solid #c6cbd1">      0.12    /     0.51     </td></tr> 
<tr style="border:1px solid #c6cbd1;background-color:#ffffff"><td style="border:1px solid #c6cbd1">  8-bit   native_single </td><td style="border:1px solid #c6cbd1">      1.00    /     1.00     </td><td style="border:1px solid #c6cbd1">      0.06    /     1.00     </td></tr> 
<tr style="border:1px solid #c6cbd1;background-color:#e6f7ff"><td style="border:1px solid #c6cbd1">  8-bit native_parallel </td><td style="border:1px solid #c6cbd1">      1.00    /     1.00     </td><td style="border:1px solid #c6cbd1">      0.02    /     0.33     </td></tr> 
<tr style="border:1px solid #c6cbd1;height:15px;background-color:#ffffff"><td></td><td></td><td></td></tr>


<tr style="border:1px solid #c6cbd1;background-color:#e6f7ff"><td style="border:1px solid #c6cbd1"> 16-bit        bitslice </td><td style="border:1px solid #c6cbd1">     33.28    /    34.52     </td><td style="border:1px solid #c6cbd1">      0.26    /     1.08     </td></tr> 
<tr style="border:1px solid #c6cbd1;background-color:#ffffff"><td style="border:1px solid #c6cbd1"> 16-bit   native_single </td><td style="border:1px solid #c6cbd1">      1.00    /     1.00     </td><td style="border:1px solid #c6cbd1">      0.13    /     1.00     </td></tr> 
<tr style="border:1px solid #c6cbd1;background-color:#e6f7ff"><td style="border:1px solid #c6cbd1"> 16-bit native_parallel </td><td style="border:1px solid #c6cbd1">      1.00    /     1.00     </td><td style="border:1px solid #c6cbd1">      0.04    /     0.33     </td></tr> 
<tr style="border:1px solid #c6cbd1;height:15px;background-color:#ffffff"><td></td><td></td><td></td></tr>


<tr style="border:1px solid #c6cbd1;background-color:#e6f7ff"><td style="border:1px solid #c6cbd1"> 32-bit        bitslice </td><td style="border:1px solid #c6cbd1">     74.55    /    73.33     </td><td style="border:1px solid #c6cbd1">      0.58    /     2.29     </td></tr> 
<tr style="border:1px solid #c6cbd1;background-color:#ffffff"><td style="border:1px solid #c6cbd1"> 32-bit   native_single </td><td style="border:1px solid #c6cbd1">      1.00    /     1.00     </td><td style="border:1px solid #c6cbd1">      0.25    /     1.00     </td></tr> 
<tr style="border:1px solid #c6cbd1;background-color:#e6f7ff"><td style="border:1px solid #c6cbd1"> 32-bit native_parallel </td><td style="border:1px solid #c6cbd1">      1.00    /     1.00     </td><td style="border:1px solid #c6cbd1">      0.08    /     0.33     </td></tr>

</table>

</center>
<br>

### Single native addition (`native_single`)

According to Intel's manual, additions (both SSE and general purpose
ones) should execute in one cycle. There should be no overhead from
the loop: looping requires incrementing a counter and doing a
conditional jump. Those two instructions get macro-fused together and
execute on port 0 or 6. The SSE addition can execute on either port 0,
1 or 5, and can thus be done at the same time as the loop
increment/jump.

Experimentally, we get a 1 cycle/iteration throughput, regardless of
the size of the addition (8-bit, 16-bit or 32-bit). On general purpose
registers, this means 1 cycle per addition. On SSE registers however,
a single `padd` instruction performs multiple additions, thus reducing
the cost per addition: 0.25 cyles for 32-bit, 0.13 cycles for 16-bit
and 0.06 cycles for 8-bit.


### Parallel native additions (`native_parallel`)

Since SSE addition can execute on either port 0, 1 or 5, three of them
could be done in a single cycle, provided that they don't have
dependencies (_i.e._ none uses the output of another one). The
increment and jump of the loop would fuse and be executed on port 6,
thus not perturbing the additions.

Once again, this corresponds to the numbers we observe experimentally:
1 cycle/iteration regarderless of the size of the additions. Since 3
`padd` instructions are executed each cycle, the cost par addition is
a third of `native_single`'s: 0.08 for 32-bit, 0.04 for 16-bit and
0.02 for 8-bit.


### Bitslice addition (`bitslice`)

Recall that the _n_-bit ripple-carry adder contains _n*5_
instructions. It relies solely on bitwise instructions, which can be
executed on ports 0, 1 and 5 on SSE registers (and on port 6 as well
on general purpose registers). The least amount of cycles this adder
can execute in is therefore _n*5/3_. However, full adders contains
inner dependencies, preventing them from execute in less than 3
cycles. Still, in an _n_-bit adder, out-of-order execution can allow
multiple full adders to execute simultaneously, thus allowing a
saturation of CPU ports 0, 1 and 5 despite the dependencies within
each full adder.

**TODO**: run `perf` to get perf counters (bottleneck?)


- bitslice:
   + total ops: 5*n. 
   + expected: assuming 3 ops/cycle -> 5n/3
     * 8-bit: exp 8*5/3=13.3, got: 14.77, close enough!
     * 16-bit: exp 16*5/3=26.5, got: 33.28
       & 32-bit: exp 32*5/3=53.3, got: 74.55
       - spilling: half the instructions are `move`


## Conclusion
