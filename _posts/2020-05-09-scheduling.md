---
layout: post
title: Scheduling
date: "2020-05-09 00:00:00"
description: Usubac's scheduling optimization
lang: en
locale: en_US
author: Darius Mercadier
excerpt: 
comments: false
hidden: true
---


## Introduction


The C codes generated by Usubac are compiled to assembly using C
compilers. While in C, a (virtually) unlimited amount of variables can
be used, Assembly can only use a few registers (between 8 and 32 for
commonly used CPUs). C variables are thus mapped to assembly registers
using a _register allocation_ algorithms. When too many registers
would be required, registers are spilled: the stack is used to
temporary store some variables. 

R. Sethi [1] showed that determining whether a program can be computed
using _k_ registers is NP-complete. Heuristic methods are thus used in
modern compilers to allocate registers to variables. The most
classical ones are based on graph coloring [2,3,4,17], and bound to be
approximative since graph coloring is NP-complete itself. Another
commonly used approach is an algorithm called _linear scan_ [6], which
just-in-time compilers often prefer to graph coloring, since it is
much faster, at the expense of the quality of the allocation
[7,8,9,10].

Register allocation is closely related to instruction scheduling,
which consists in ordering the instructions in the best way to take
advantage of a CPU's superscalar microarchitecture [11,12]. To
illustrate the importance of instruction scheduling, consider the
following instructions:

```c
u = a + b;
v = u + 2;
w = c + d;
x = e + f;
```

Consider a CPU that can compute two additions per cycle and performs
no reordering. Such a CPU would execute `u = a + b` in its first
cycle, and would be unable to compute `v = u + 2` in the same cycle
since `u` has not been computed yet. In a second cycle, it would
compute `v = u + 2` as well as `w = c + d`, and, finally, in a third
cycle, it would compute `x = e + f`. On the other hand, if the code
had been scheduled as:

```c
u = a + b;
w = c + d;
v = u + 2;
x = e + f;
```

It could have been executed in only 2 cycles. While out-of-order CPUs
reduce the impact of such data hazards, this phenomenons still occur
and need to be taken into account to schedule instructions.

The interaction between register allocation and instruction scheduling
is due to spilling, which introduces memory operations. The cost of
those additional memory operations can sometimes be reduced by using a
efficient instruction scheduling. Better yet, it is well known that
combining register allocation and instruction scheduling can produce
more efficient code that performing them separately [13,14,15,16].

In practice however, modern C compilers force themselves to be fast,
sometimes at the expense of the quality of the generated code. Both
GCC and LLVM perform instruction scheduling and register allocation
separately, starting instruction scheduling, followed by register
allocation, followed by some additional scheduling.

Despite being based on traditional algorithms, GCC and LLVM's
instruction schedulers are register allocators are highly optimized,
and are the product of decades of tweaking. GCC's register allocator
and instruction scheduler thus amount for more than 50.000 lines of C
code, and LLVM's for more than 30.000 lines of code.

Recreating from scratch a better register allocator and instruction
scheduler than Clang and GCC thus seem utopian. Instead, we developped
two schedulers for C code, that aim at helping GCC/LLVM's schedulers
and register allocator. The first scheduler reduces register pressure
in bitsliced ciphers, while the second increased instruction-level
parallelism in msliced ciphers.



## Bitslicing


In bitsliced codes, the major bottleneck is register pressure: a
significant portion of the execution time is spent spilling registers
to and from the stack. Given that hundreds of variables can be alive
at the same time in a bitslice cipher, it is not surprising that the C
compilers have a hard time keeping the register pressure down. For
instance, it is common for bitsliced ciphers to have up to 40% of
their instructions being loads and stores for spilling, as shown in
the following table (usuba-generated ciphers compiled with Clang
7.0.0):

<style type="text/css">
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  overflow:hidden;padding:10px 5px;word-break:normal;}
.tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}
.tg .tg-ggg6{background-color:#ecf4ff;text-align:center;vertical-align:middle}
.tg .tg-wa1i{font-weight:bold;text-align:center;vertical-align:middle}
.tg .tg-yla0{font-weight:bold;text-align:left;vertical-align:middle}
.tg .tg-0qe0{background-color:#ecf4ff;text-align:left;vertical-align:top}
.tg .tg-0lax{text-align:left;vertical-align:top}
.tg .tg-nrix{text-align:center;vertical-align:middle}
</style>
<center>
<table class="tg" style="undefined;table-layout: fixed; width: 304px; margin-top:30px;margin-bottom:30px">
<colgroup>
<col style="width: 152px">
<col style="width: 152px">
</colgroup>
<thead>
  <tr>
    <th class="tg-yla0">Cipher</th>
    <th class="tg-wa1i">% of instructions related to spilling</th>
  </tr>
</thead>
<tbody>
  <tr>
    <td class="tg-0qe0">DES</td>
    <td class="tg-ggg6">41%</td>
  </tr>
  <tr>
    <td class="tg-0lax">Ascon</td>
    <td class="tg-nrix">43%</td>
  </tr>
  <tr>
    <td class="tg-0qe0">Gift</td>
    <td class="tg-ggg6">38%</td>
  </tr>
  <tr>
    <td class="tg-0lax">Present</td>
    <td class="tg-nrix">25%</td>
  </tr>
  <tr>
    <td class="tg-0qe0">Rectangle</td>
    <td class="tg-ggg6">36%</td>
  </tr>
  <tr>
    <td class="tg-0lax">Serpent</td>
    <td class="tg-nrix">39%</td>
  </tr>
</tbody>
</table>
</center>


We designed a scheduling algorithm that aims at reducing register
pressure (and thus improve performances) in bitsliced codes.

Let's take the example of Rectangle to illustrate why bitsliced codes
have so much spilling and how this can be improved. Rectangle's S-box
can be written in Usuba as:

```lustre
node sbox (a0:u32, a1:u32, a2:u32, a3:u32)
     returns  (b0:u32, b1:u32, b2:u32, b3:u32)
vars
    t1:u32, t2:u32, t3:u32, t4:u32, t5:u32, t6:u32,
    t7:u32, t8:u32, t9:u32, t10:u32, t11:u32, t12:u32
let
    t1 = ~a1;
    t2 = a0 & t1;
    t3 = a2 ^ a3;
    b0 = t2 ^ t3;
    t5 = a3 | t1;
    t6 = a0 ^ t5;
    b1 = a2 ^ t6;
    t8 = a1 ^ a2;
    t9 = t3 & t6;
    b3 = t8 ^ t9;
    t11 = b0 | t8;
    b2 = t6 ^ t11
tel
```

Usubac naive automatic bitslicing would replace all `u32` with `b32`,
which would cause the variables to become vectors of 32 boolean
elements, thus causing the operators to be unfolded as follows:

```lustre
node sbox (a0:b32, a1:b32, a2:b32, a3:b32)
     returns  (b0:b32, b1:b32, b2:b32, b3:b32)
vars ...
let
    t1[0] = ~a1[0];
    t1[1] = ~a1[1];
    t1[2] = ~a1[2];
    ...
    t1[31] = ~a1[31];
    t2[0] = a0[0] & t1[0];
    t2[1] = a0[1] & t1[1];
    ...
    t2[31] = a0[31] & t1[31];
    ...
```

### Optimizing the automatic bitslicing

While the initial S-box contained less than 10 variables
simultaneously alive, the bitsliced S-box contains 32 times more
variables alive at any point. A first optimization to reduce the
register pressure thus happens during the automatic bitslicing: Usubac
detects nodes computing only bitwise instructions and calls to nodes
with the same property. Those nodes are specialized to take `b1`
variables as input, and their call sites are replaced with several
calls instead of one. In the case of Rectangle, the shorter S-box is
thus called 32 times rather than calling the large S-box
once. Concretely, this optimization reduces the register pressure, at
the expanse of more function calls. The benefits heavily depends on
the ciphers and C compilers used. On AVX registers using Clang as C
compiler, this yields a 34% speedup on Ascon, 12% on Gift and 6% on
Clyde.


The same optimization cannot be applied to linear layers however, as
they almost always contain rotations, shifts or permutations. Consider
for instance Rectangle's linear layer:

```lustre
node ShiftRows (input:u16x4) returns (out:u16x4)
let
    out[0] = input[0];
    out[1] = input[1] <<< 1;
    out[2] = input[2] <<< 12;
    out[3] = input[3] <<< 13
tel
```

Using a `u1x4` input instead of a `u16x4` is not possible due to the
left rotations, which requires all 16 bits of each input. Instead,
this function will be bitsliced and become:

```lustre
node ShiftRows (input:b1[4][16]) returns (out:b1[4][16])
let 
    out[0][0] = input[0][0];       -- out[0] = input[0]
    out[0][1] = input[0][1];
    ...
    out[0][15] = input[0][15];
    out[1][0] = input[1][1];       -- out[1] = input[1] <<< 1
    out[1][1] = input[1][2];
    ...
    out[1][15] = input[1][0];
    out[2][0] = input[2][12];      -- out[2] = input[2] <<< 12
    out[2][1] = input[2][13];
    ...
    out[2][15] = input[2][11];
    out[3][0] = input[3][13];      -- out[3] = input[3] <<< 13
    out[3][1] = input[3][14];
    ...
    out[3][15] = input[3][12];
tel
```

In the case of Rectangle's `ShiftRows` function, it can be inlined and
entirely removed using copy propagation since it only contains
assignments. However, for some other ciphers like Ascon, Serpent,
Clyde or Skinny, the linear layers contains `xor`s, which cannot be
optimized away. Similarly, the `AddRoundKey` step of most ciphers
introduces a lot of `xor`s.

Overall, after bitslicing, the main function of Rectangle thus looks
like:

```lustre
state := AddRoundKey(state, key[0]);    -- <--- lots of spilling in this node
state[0..3] := Sbox(state[0..3]);
state[4..7] := Sbox(state[4..7]);
...
state[60..63] := Sbox(state[60..63]);
state := LinearLayer(state);            -- <--- lots of spilling in this node
state := AddRoundKey(state, key[1]);    -- <--- lots of spilling in this node
state[0..3] := Sbox(state[0..3]);
state[4..7] := Sbox(state[4..7]);
...
state[60..63] := Sbox(state[60..63]);
...
```


### Bitslice code scheduling

We designed a scheduling algorithm that aims at interleaving the
linear layers (and key additions) with S-box calls in order to reduce
the live ranges of some variables and thus reduce the need for
spilling.

This algorithm requires a first inlining step that heuristically tries
to recognize good candidates for inlining. We experimentally
determined two criterions for this first inlining pass that yield good
results:

 - if a node contains more than than 31 inputs and outputs, it's very
   unlikely to be an S-box and should therefore be inlined. The
   largest S-boxes we know of take 8 inputs and return 8 outputs, like
   in AES or Camellia.
   
 - if a node contains more than 65% of assigmnents, it is quite likely
   to be doing some kind of permutation, and should therefore be
   inlined. Since most of the instructions in S-boxes are bitwise
   instructions, a node containing 65% is thus unlikely to be an
   S-box.

After this inlining step, Rectangle's code would thus be:

```lustre
state[0] := state[0] ^ key[0][0];        -- AddRoundKey 1
state[1] := state[1] ^ key[0][1];
state[2] := state[2] ^ key[0][2];
state[3] := state[3] ^ key[0][3];
state[4] := state[4] ^ key[0][4];
state[5] := state[5] ^ key[0][5];
state[6] := state[6] ^ key[0][6];
state[7] := state[7] ^ key[0][7];
state[8] := state[8] ^ key[0][8];
state[9] := state[9] ^ key[0][9];
...
state[63] := state[63] ^ key[0][63];
state[0..3] := Sbox(state[0..3]);        -- S-boxes 1
state[4..7] := Sbox(state[4..7]);
state[8..11] := Sbox(state[8..11]);
...
state[60..63] := Sbox(state[60..63]);
state[0] := state[0];                    -- Linear layer 1
state[1] := state[1];
state[2] := state[2];
state[3] := state[3];
state[4] := state[4];
state[5] := state[5];
state[6] := state[6];
...
state[63] := state[60];
state[0] := state[0] ^ key[1][0];        -- AddRoundKey 2
state[1] := state[1] ^ key[1][1];
state[2] := state[2] ^ key[1][2];
state[3] := state[3] ^ key[1][3];
state[4] := state[4] ^ key[1][4];
state[5] := state[5] ^ key[1][5];
...
state[63] := state[63] ^ key[1][63];
state[0..3] := Sbox(state[0..3]);        -- S-boxes 2
state[4..7] := Sbox(state[4..7]);
...
```

The inlined inlined linear and key addition introduce a lot of
spilling since they use a lot of variables a single time. After our
scheduling algorithm, Rectangle's code will be:


```lustre
state[0] := state[0] ^ key[0][0];        -- Part of AddRoundKey 1
state[1] := state[1] ^ key[0][1];
state[2] := state[2] ^ key[0][2];
state[3] := state[3] ^ key[0][3];
state[0..3] := Sbox(state[0..3]);        -- S-box 1
state[4] := state[4] ^ key[0][4];        -- Part of AddRoundKey 1
state[5] := state[5] ^ key[0][5];
state[6] := state[6] ^ key[0][6];
state[7] := state[7] ^ key[0][7];
state[4..7] := Sbox(state[4..7]);        -- S-box 1
...
...
state[0] := state[0];                    -- Part of Linear layer 1
state[1] := state[1];
state[2] := state[2]; 
state[3] := state[3];
state[0] := state[0] ^ key[1][0];        -- Part of AddRoundKey 2
state[1] := state[1] ^ key[1][1];
state[2] := state[2] ^ key[1][2];
state[3] := state[3] ^ key[1][3];
state[0..3] := Sbox(state[0..3]);        -- S-box 2
state[4] := state[4];                    -- Part of Linear layer 1
state[5] := state[5];
state[6] := state[6]; 
state[7] := state[7];
state[4] := state[4] ^ key[1][4];        -- Part of AddRoundKey 2
state[5] := state[5] ^ key[1][5];
state[6] := state[6] ^ key[1][6];
state[7] := state[7] ^ key[1][7];
state[4..7] := Sbox(state[0..3]);        -- S-box 2
...
...
```

Our scheduling algorithm aims at reducing the lifespan of variables
computed in the linear layers by interleaving linear layer with
S-boxes calls: the parameters of the S-boxes are computed right before
the S-box calls, thus removing the need to spill them. More formally,
the pseudo-code of this algorithm follows:

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">procedure</span> <span class="nf">Schedule</span><span class="p">(</span><span class="mi"><i>prog</i></span><span class="p">):</span>
    <span class="o">for</span> <span class="o">each</span> <span class="n">function</span> <span class="n">call</span> <span class="mi"><i>funCall</i></span> <span class="n">of</span> <span class="mi"><i>prog</i></span> <span class="o">do</span>
        <span class="o">for</span> <span class="o">each</span> <span class="n">variable</span> <span class="mi"><i>v</i></span> <span class="n">in</span> <span class="mi"><i>funCall</i></span><span class="n">'s</span> <span class="n">arguments</span> <span class="o">do</span>
            <span class="o">if</span> <span class="mi"><i>v</i></span><span class="n">'s</span> <span class="n">definition</span> <span class="n">is</span> <span class="n">not</span> <span class="n">scheduled</span> <span class="n">yet</span> <span class="o">then</span>
                <span class="n"><b>schedule</b></span> <span class="mi"><i>v</i></span><span class="n">'s</span> <span class="n">definition</span> <span class="p">(</span><span class="n">and</span> <span class="n">dependencies</span><span class="p">)</span> <span class="nb">next</span>
        <span class="n"><b>schedule</b></span> <span class="mi"><i>funCall</i></span> <span class="nb">next</span>
</code></pre></div></div>

<!--         <span class="o">for</span> <span class="o">each</span> <span class="n">variable</span> <span class="mi"><i>v</i></span> <span class="n">defined</span> <span class="n">by</span> <span class="mi"><i>funCall</i></span> <span class="o">do</span> -->
<!--             <span class="o">for</span> <span class="o">each</span> <span class="n">equation</span> <span class="mi"><i>eqn</i></span> <span class="n">of</span> <span class="mi"><i>prog</i></span> <span class="n">using</span> <span class="mi"><i>v</i></span> <span class="o">do</span> -->
<!--                 <span class="o">if</span> <span class="mi"><i>eqn</i></span> <span class="n">is</span> <span class="n">ready</span> <span class="n">to</span> <span class="n">be</span> <span class="n">scheduled</span> <span class="o">then</span> -->
<!--                     <span class="n"><b>schedule</b></span> <span class="mi"><i>eqn</i></span> <span class="nb">next</span> -->
<!-- </code></pre></div></div> -->


The optimization of the automatic bitslicing presented earlier is
crucial for this scheduling algorithm to work: without it, the S-box
is a single large function rather than several calls to small S-boxes,
and no interleaving can happen. Similarly, the inlining pass than
happens before the scheduling itself is essential to ensure that the
instructions of the linear layer are inlined and ready to be
interleaved with calls to the S-boxes.


### Performances

We evaluated this algorithm on 11 ciphers containing a distinct S-box
and linear layer, where the S-box only performs bitwise
instructions. We compiled the generated C codes with gcc 8.3, and ran
the benchmarks on a Intel i5 6500. The results are shown in the
following table:

<center>
<style type="text/css">
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  overflow:hidden;padding:10px 5px;word-break:normal;}
.tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}
.tg .tg-ti2t{color:#32cb00;text-align:left;vertical-align:top}
.tg .tg-cjtp{background-color:#ecf4ff;border-color:inherit;text-align:left;vertical-align:top}
.tg .tg-gyiu{background-color:#ecf4ff;color:#32cb00;text-align:left;vertical-align:top}
.tg .tg-3u1j{background-color:#ecf4ff;border-color:#000000;color:#32cb00;text-align:left;vertical-align:top}
.tg .tg-wa1i{font-weight:bold;text-align:center;vertical-align:middle}
.tg .tg-aznb{background-color:#ecf4ff;font-style:italic;text-align:left;vertical-align:top}
.tg .tg-66qz{background-color:#ffffff;border-color:#000000;color:#32cb00;text-align:left;vertical-align:top}
.tg .tg-viqs{color:#fe0000;text-align:left;vertical-align:top}
.tg .tg-uzvj{border-color:inherit;font-weight:bold;text-align:center;vertical-align:middle}
.tg .tg-amwm{font-weight:bold;text-align:center;vertical-align:top}
.tg .tg-8zwo{font-style:italic;text-align:left;vertical-align:top}
.tg .tg-0lax{text-align:left;vertical-align:top}
.tg .tg-l2sb{background-color:#ecf4ff;color:#fe0000;text-align:left;vertical-align:top}
.tg .tg-c6of{background-color:#ffffff;border-color:inherit;text-align:left;vertical-align:top}
.tg .tg-7vke{background-color:#ffffff;color:#32cb00;text-align:left;vertical-align:top}
.tg .tg-0qe0{background-color:#ecf4ff;text-align:left;vertical-align:top}
.tg .tg-0pky{border-color:inherit;text-align:left;vertical-align:top}
.tg .tg-085k{border-color:#000000;color:#32cb00;text-align:left;vertical-align:top}
</style>
<table class="tg" style="undefined;table-layout: fixed; width: 725px; margin-top:30px;margin-bottom:30px">
<colgroup>
<col style="width: 125px">
<col style="width: 100px">
<col style="width: 100px">
<col style="width: 100px">
<col style="width: 100px">
<col style="width: 100px">
<col style="width: 100px">
</colgroup>
<thead>
  <tr>
    <th class="tg-uzvj" rowspan="3">algorithm</th>
    <th class="tg-uzvj" colspan="6">Bitslice scheduling speedup</th>
  </tr>
  <tr>
    <td class="tg-wa1i" colspan="2">gcc -Os</td>
    <td class="tg-wa1i" colspan="2">gcc -O3</td>
    <td class="tg-wa1i" colspan="2">clang -O3</td>
  </tr>
  <tr>
    <td class="tg-uzvj">x86</td>
    <td class="tg-uzvj">AVX2</td>
    <td class="tg-amwm">x86</td>
    <td class="tg-amwm">AVX2</td>
    <td class="tg-amwm">x86</td>
    <td class="tg-amwm">AVX2</td>
  </tr>
</thead>
<tbody>
  <tr>
    <td class="tg-8zwo">ACE*</td>
    <td class="tg-0lax">1.00</td>
    <td class="tg-0lax">1.00</td>
    <td class="tg-ti2t">1.02</td>
    <td class="tg-ti2t">1.01</td>
    <td class="tg-0lax">1.00</td>
    <td class="tg-viqs">0.99</td>
  </tr>
  <tr>
    <td class="tg-cjtp">AES</td>
    <td class="tg-3u1j">1.04</td>
    <td class="tg-l2sb">0.99</td>
    <td class="tg-gyiu">1.06</td>
    <td class="tg-l2sb">0.98</td>
    <td class="tg-gyiu">1.08</td>
    <td class="tg-gyiu">1.04</td>
  </tr>
  <tr>
    <td class="tg-c6of">Ascon</td>
    <td class="tg-66qz">1.35</td>
    <td class="tg-66qz">1.32</td>
    <td class="tg-7vke">1.25</td>
    <td class="tg-7vke">1.27</td>
    <td class="tg-7vke">1.04</td>
    <td class="tg-7vke">1.19</td>
  </tr>
  <tr>
    <td class="tg-cjtp">Clyde</td>
    <td class="tg-3u1j">1.06</td>
    <td class="tg-3u1j">1.06</td>
    <td class="tg-gyiu">1.06</td>
    <td class="tg-gyiu">1.04</td>
    <td class="tg-gyiu">1.01</td>
    <td class="tg-l2sb">0.99</td>
  </tr>
  <tr>
    <td class="tg-c6of">DES</td>
    <td class="tg-66qz">1.16</td>
    <td class="tg-66qz">1.19</td>
    <td class="tg-7vke">1.16</td>
    <td class="tg-7vke">1.23</td>
    <td class="tg-7vke">1.01</td>
    <td class="tg-7vke">1.02</td>
  </tr>
  <tr>
    <td class="tg-aznb">Gimli*</td>
    <td class="tg-0qe0">1.00</td>
    <td class="tg-l2sb">0.99</td>
    <td class="tg-gyiu">1.01</td>
    <td class="tg-0qe0">1.00</td>
    <td class="tg-0qe0">1.00</td>
    <td class="tg-gyiu">1.01</td>
  </tr>
  <tr>
    <td class="tg-0pky">Gift</td>
    <td class="tg-085k">1.16</td>
    <td class="tg-085k">1.18</td>
    <td class="tg-ti2t">1.12</td>
    <td class="tg-ti2t">1.16</td>
    <td class="tg-ti2t">1.04</td>
    <td class="tg-ti2t">1.09</td>
  </tr>
  <tr>
    <td class="tg-cjtp">Photon</td>
    <td class="tg-3u1j">1.05</td>
    <td class="tg-3u1j">1.14</td>
    <td class="tg-l2sb">0.97</td>
    <td class="tg-l2sb">0.93</td>
    <td class="tg-l2sb">0.96</td>
    <td class="tg-l2sb">0.97</td>
  </tr>
  <tr>
    <td class="tg-0pky">Present</td>
    <td class="tg-085k">1.30</td>
    <td class="tg-085k">1.10</td>
    <td class="tg-ti2t">1.16</td>
    <td class="tg-ti2t">1.16</td>
    <td class="tg-0lax">1.00</td>
    <td class="tg-0lax">1.00</td>
  </tr>
  <tr>
    <td class="tg-cjtp">Pyjamask</td>
    <td class="tg-3u1j">1.19</td>
    <td class="tg-3u1j">1.35</td>
    <td class="tg-gyiu">1.04</td>
    <td class="tg-gyiu">1.04</td>
    <td class="tg-l2sb">0.99</td>
    <td class="tg-0qe0">1.00</td>
  </tr>
  <tr>
    <td class="tg-0pky">Rectangle</td>
    <td class="tg-085k">1.28</td>
    <td class="tg-085k">1.20</td>
    <td class="tg-ti2t">1.15</td>
    <td class="tg-ti2t">1.15</td>
    <td class="tg-0lax">1.00</td>
    <td class="tg-viqs">0.99</td>
  </tr>
  <tr>
    <td class="tg-cjtp">Serpent</td>
    <td class="tg-3u1j">1.18</td>
    <td class="tg-3u1j">1.20</td>
    <td class="tg-gyiu">1.20</td>
    <td class="tg-gyiu">1.20</td>
    <td class="tg-gyiu">1.04</td>
    <td class="tg-0qe0">1.00</td>
  </tr>
  <tr>
    <td class="tg-0pky">Skinny</td>
    <td class="tg-085k">1.14</td>
    <td class="tg-085k">1.16</td>
    <td class="tg-ti2t">1.18</td>
    <td class="tg-ti2t">1.18</td>
    <td class="tg-ti2t">1.03</td>
    <td class="tg-ti2t">1.14</td>
  </tr>
  <tr>
    <td class="tg-aznb">Spongent*</td>
    <td class="tg-0qe0">1.00</td>
    <td class="tg-0qe0">1.00</td>
    <td class="tg-0qe0">1.00</td>
    <td class="tg-gyiu">1.02</td>
    <td class="tg-0qe0">1.00</td>
    <td class="tg-l2sb">0.99</td>
  </tr>
  <tr>
    <td class="tg-8zwo">Subterranean*</td>
    <td class="tg-0lax">1.00</td>
    <td class="tg-viqs">0.99</td>
    <td class="tg-0lax">1.00</td>
    <td class="tg-ti2t">1.01</td>
    <td class="tg-0lax">1.00</td>
    <td class="tg-0lax">1.00</td>
  </tr>
  <tr>
    <td class="tg-aznb">Xoodoo*</td>
    <td class="tg-0qe0">1.00</td>
    <td class="tg-gyiu">1.01</td>
    <td class="tg-0qe0">1.00</td>
    <td class="tg-0qe0">1.00</td>
    <td class="tg-l2sb">0.98</td>
    <td class="tg-0qe0">1.00</td>
  </tr>
</tbody>
</table>
</center>


Our scheduling algorithm is clearly beneficial, yielding speedups of
up to x1.35. The ciphers marked with a * are not supposed to benefit
from this scheduling algorithm, since they do not contain a small
S-box composed of bitwise instructions alternating with a large linear
layer. We still provide their performances to show that our algorithm
is not too detrimental to them: on average, it does not change their
performances. Note that all the speedups are significant (even a x1.01
speedup) according to Wilcoxon's rank-sum test [19]. 

The exact benefits of this scheduling algorithm however greatly vary
from one cipher to the other, from one architecture to the other, and
from one compiler to the other. For instance, while on general purpose
register our algorithm improves AES's performances by a factor 1.04,
it actually reduces them by 0.99 on AVX2 registers. On Present, our
algorithm is clearly more efficient on general purpose registers,
while on Pyjamask, it is clearly better on AVX2 registers.

We do not provide a detailed analysis of the performances because of
the heuristic nature the results: our algorithm relies on a heuristic
inlining, and is only meant to help GCC/Clang's scheduler, which are
themselves very heuristic.



## mslicing


Unlike bitsliced code, m-sliced programs have much lower register
pressure. Spilling is less of an issue, the latency of the few
resulting load and store operations being hidden by the CPU
pipeline. Instead, the challenge consists in being able to saturate
the parallel CPU execution units. To do so, one must increase ILP,
taking into account the availability of specialized execution
units. For instance, hsliced code will rely on SIMD shuffle
instructions (vpshufb) that can be executed on a single execution unit
on current Skylake architecture. Performing a sequence of shuf- fles
is extremely detrimental to performance: the execution of the shuffles
(and their respective dependencies) become serialized, bottlenecking
the dedicated execution unit.  One might think that the out-of-order
nature of modern CPU would alleviate this issue, allowing the
processor to execute independent instructions ahead in the execution
streams. C compilers (such as ICC and Clang) seems to adopt this
policy: groups of shuffle in the source code will be scheduled as-is
in the resulting assembly. Only GCC statically schedules independent
(for example, arithmetic) instructions in-between shuffles in the
generated assembly. However, due to the bulky nature of sliced code,
we observe that the reservation station is quickly saturated,
preventing actual out-of-order execution.  We statically increase ILP
in Usubac by maintaining a look-behind window of the previous 16
instructions (which corresponds to the maximal number of registers
available on Intel platforms without AVX512) while scheduling. To
schedule an instruction, we search among the remaining instructions
one with no data hazard with the instructions in PLDI ’19, June 22–26,
2019, Phoenix, AZ, USA the look-behind window, and that would execute
on a differ- ent execution unit. If no such instruction can be found,
we reduce the size of the look-behind window, and, utlimately just
schedule any instruction that is ready.  This scheduling algorithm
increased the throughput of hsliced AES by 2.43%, and of vsliced
Chacha20 by 9.09%.  Inspecting the generated assembly, we notice that,
in both cases, the impact of data hazards have been minimized by
reorganizing computations within each rounds.  Inlining is
instrumental in improving the quality of sched- uling in this setting
too: it allows instructions to flow freely across node calls. For
instance, a round of Chacha20 is spec- ified in terms of 2
half-rounds, which we naturally imple- ment with two nodes. Thanks to
inlining, we can afford to define a node, hence increasing
readability, knowing that, performance-wise, it is transparent.
Whereas inlining allows scheduling to improve the code quality within
a single round, symmetric ciphers usually combine ten or more
iterations of a given round. For instance, Chacha20 performs 10
iterations of a double round, and AES performs between 10 and 14
rounds. In Usuba, these iterations are naturally expressed with a
grouped definition, using the forall declaration.  Usubac can choose
between expanding the forall dec- laration – unrolling the loop at the
expense of code size – or to translate it into a C for loop. The loop
bounds being statically known, the C compiler could also decide to
fully unroll this loop. However, the size of the loop body being
significant, compilers prefer to avoid unrolling it, a sound decision
in general since code locality ought to be preserved.  Since we have
perfect locality anyway, code size is not an issue in our setting. We
may thus profitably unroll all the rounds into a single straight-line
program: scheduling is thus able to re-order instructions across
distinct encryption rounds. On AES (resp. Chacha20), this yields a
3.22% (resp.  3.63%) speedup compared to an implementation performing
intra-round scheduling only, at the expense of a 31.90% (resp.
19.40%) increase in code size.


---
## References

[1] R. Sethi, [Complete Register Allocation Problems](https://epubs.siam.org/doi/pdf/10.1137/0204020?casa_token=NVMheBw109IAAAAA%3Ae86ZIKaHkzbEDSYAIJgKzx96NA9ibs9i4nRT2uepjdzHB45wvIgYPQwjxacwkUwapsZREwdNJ8A&), STOC, 1973.

[2] G. J. Chaitin _et al._, [Register allocation via coloring](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.452.8606&rep=rep1&type=pdf), 1981.

[3] G. J. Chaitin, [Register allocation & spilling via graph coloring](https://dl.acm.org/doi/pdf/10.1145/872726.806984), CC, 1982.

[4] P. Briggs _et al._, [Coloring heuristics for register allocation](https://dl.acm.org/doi/pdf/10.1145/74818.74843), PLDI, 1989.

[5] P. Briggs _et al._, [Improvements to graph coloring register allocation](https://dl.acm.org/doi/pdf/10.1145/177492.177575), 1994.

[6] M. Poletto, V. Sarkar, [Linear scan register allocation](https://dl.acm.org/doi/pdf/10.1145/330249.330250), 1999.

[7] B. Alpern _et al._, [The Jalapeño virtual machine](https://pdfs.semanticscholar.org/31e4/5feb1fcc0bd65991e814c68d601402efece2.pdf), 2000.

[8] T. Kotzmann _et al._, [Design of the Java HotSpot client compiler for Java 6](https://dl.acm.org/doi/pdf/10.1145/1369396.1370017), 2008.

[9] C. Wimmer, M. Franz, [Linear scan register allocation on SSA form](https://dl.acm.org/doi/pdf/10.1145/1772954.1772979), CGO, 2010.

[10] H. Mössenböck, M. Pfeiffer, [Linear Scan Register Allocation in the Context of SSA Form and Register Constraints](https://link.springer.com/content/pdf/10.1007/3-540-45937-5_17.pdf), CC, 2002.

[11] P. B. Gibbons, S. S. Muchnick, [Efficient instruction scheduling for a pipelined architecture](https://dl.acm.org/doi/pdf/10.1145/12276.13312), CC, 1986.

[12] D. Bernstein, M. Rodeh, [Global instruction scheduling for superscalar machines](https://dl.acm.org/doi/pdf/10.1145/113445.113466), PLDI, 1991.

[13] R. F. Touzeau, [A Fortran compiler for the FPS-164 scientific computer](https://dl.acm.org/doi/pdf/10.1145/502949.502879), CC, 1984.

[14] J. R. Goodman, W.-C. Hsu, [Code scheduling and register allocation in large basic blocks](https://dl.acm.org/doi/pdf/10.1145/2591635.2667158), ICS, 1988.

[15] S. S. Pinter, [Register allocation with instruction scheduling: a new approach](https://dl.acm.org/doi/pdf/10.1145/155090.155114), PLDI, 1993.

[16] R. Motwani _et al._, [Combining register allocation and instruction scheduling](http://i.stanford.edu/pub/cstr/reports/cs/tn/95/22/CS-TN-95-22.pdf), 1995.

[17] M. D. Smith, G. Holloway, [Graph-Coloring Register Allocation for Irregular Architectures](https://www.cs.tufts.edu/comp/150FP/archive/mike-smith/irreg.pdf), 2001.

[18] V. N. Makarov, [The Integrated Register Allocator for GCC](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.66.1330&rep=rep1&type=pdf#page=77), 2007.

[19] F. Wilcoxon, [Individual comparisons by ranking methods](https://link.springer.com/content/pdf/10.1007%2F978-1-4612-4380-9.pdf), 1992.
