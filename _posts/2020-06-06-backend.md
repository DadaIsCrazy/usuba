---
layout: post
title: Usubac - backend
date: "2020-06-06 00:00:00"
description: The backend of Usuba
lang: en
locale: en_US
author: Darius Mercadier
excerpt: 
comments: false
hidden: true
---


Usubac's backend is responsible of optimizing the Usuba0 code and
utlimately generating C code. Masking is also done in the backend, but
will be presented in a later post.

Generating C code allows us to partially rely on C compiler's
optimizer to improve the performances of the generated
ciphers. However, that alone would not be sufficient to achieve
similar performances as carefuly hand-tuned codes.

We divide our optimizations in two categories. The simple ones (common
subexpression elimination, inlining, unrolling...) are already done by
most C compilers, but we still perform them in Usuba, mainly in order
to improve the effectiveness of the more advanced ones, but also to
not rely too much on the C compiler's optimizers heuristic, which may
not be tailored for cryptographic codes. The more advanced
optimizations include two [scheduling algorithms]({{ site.baseurl}} {%
post_url 2020-05-09-scheduling %}), and an [interleaving pass]({{
site.baseurl}} {% post_url 2020-03-16-interleaving %}), which are
presented in separate posts.

Those advanced optimizations can only be done thanks to the knowledge
we have in Usuba regarding the codes we are dealing with (ciphers),
which C compilers do not have. One of our scheduling algorithm is thus
tailored to reduce the spilling in linear layers of bitsliced ciphers,
while the other one improves instruction level parallelism by mixing
linear layers and S-boxes of msliced ciphers.

Usuba's dataflow programming model is also key for our interleaving
optimization: since Usuba manipulates streams rather than scalars, we
are able to increase instruction level parallelism by processing
several elements of the input stream simultaneously.



### Common Subexpression Elimination, Copy Propagation, Constant Folding

Common subexpression elimination (CSE) is a classical optimization
that aim at preventing identical expressions to be computed multiple
times. When an expression that has already been computed is
recomputed, it is instead replaced by the previously computed
value. For instance,

```c
x = a + b;
y = a + b;
```

would be transformed by CSE into

```c
x = a + b;
y = x;
```

Copy propagation is also a traditional optimization that removes
assignments of a variable into another variable. For instance,

```c
x = a + b;
y = x;
return y;
```

would be transformed by CP into

```c
x = a + b;
return x;
```

Finally, constant folding consists in computing constant expressions
at compile time. The expressions that Usuba simplifies using constant
folding are either arithmetic or bitwise operations between constants
(_eg._ replacing `x = 20 + 22` by `x = 42`) or bitwise operations
whose operand is either `0` or `0xffffffff` (_eg._ replacing `x = a |
0` by `x = a`).

CSE, copy propagation and constant folding are already done by C
compilers. However, performing them in Usubac has two main benefits:

 - It produces smaller C codes, often with very little cost to
   readability. A non-negligible part of the copies removed by copy
   propagation comes from temporary variables introduced by the
   compiler itself. Removing such assignments actually improves
   readability. This matters especially in bitsliced codes, which can
   contain tens of thousands lines of C code after those
   optimizations, and that would contain hundreds of thousands lines
   without them. For instance, ACE bitsliced is about 200.000
   instructions without those optimizations but only 50.000 with them.
   
 - It makes the [scheduling optimizations]({{ site.baseurl}}{%
   post_url 2020-05-09-scheduling %}) more efficient. Needless
   assignments and redundant expressions increase the number of live
   variables, which may throw off the schedulers. 

### Unrolling


Normalization: Needed in some cases:

   - shifts/rotations with index in bitslicing (example: gift (better one?))
   
   - Use of array of nodes (example: Serpent/DES)

Optimization: 
 
  - Small loops: 
  
    + always better to unroll (C compilers do that anyway), cause
      overhead of looping

  - Small + larger loops:
  
    + Better to unroll to allow interleaving
    
  - Hard to put an upper bound above which no need to unroll: 
  
    + Pyjamask matrix multiplication: 5x32 = 160 instrs but better to
      unroll in order to interleave (x1.68 with unrolling).


### Inlining

The decision of inlining nodes is partly justified by the usual
reasoning applied by C compilers: a function call implies a
significant overhead that, for very frequently called functions (such
as S-boxes, in our case) makes it worth the increase in code
size. However, Usuba's [m-sliched code scheduling algorithm]({{
site.baseurl}}{% post_url 2020-05-09-scheduling %}) requires some
inlining since one of its effects is to interleave nodes to increase
instruction level parallelism. We thus perform some inlining in
Usuba. The following table shows the speedups gained by inlining all
nodes on some msliced ciphers, compared to inlining none:

<style type="text/css">
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  overflow:hidden;padding:10px 5px;word-break:normal;}
.tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}
.tg .tg-18eh{border-color:#000000;font-weight:bold;text-align:center;vertical-align:middle}
.tg .tg-65px{background-color:#ecf4ff;border-color:#000000;text-align:left;vertical-align:top}
.tg .tg-wsl5{border-color:#000000;color:#fe0000;text-align:left;vertical-align:top}
.tg .tg-3u1j{background-color:#ecf4ff;border-color:#000000;color:#32cb00;text-align:left;vertical-align:top}
.tg .tg-mqa1{border-color:#000000;font-weight:bold;text-align:center;vertical-align:top}
.tg .tg-73oq{border-color:#000000;text-align:left;vertical-align:top}
.tg .tg-085k{border-color:#000000;color:#32cb00;text-align:left;vertical-align:top}
.tg .tg-376w{background-color:#ecf4ff;border-color:#000000;color:#fe0000;text-align:left;vertical-align:top}
</style>

<center>
<table class="tg" style="undefined;table-layout: fixed; width: 614px; margin-top:30px;margin-bottom:30px">
<colgroup>
<col style="width: 156px">
<col style="width: 115px">
<col style="width: 115px">
<col style="width: 114px">
<col style="width: 114px">
</colgroup>
<thead>
  <tr>
    <td class="tg-mqa1" colspan="5">mslicing</td>
  </tr>
  <tr>
    <th class="tg-18eh" rowspan="3">Cipher</th>
    <th class="tg-18eh" colspan="4">Inlining speedup</th>
  </tr>
  <tr>
    <td class="tg-18eh" colspan="2">clang</td>
    <td class="tg-18eh" colspan="2">gcc</td>
  </tr>
  <tr>
    <td class="tg-18eh">x86</td>
    <td class="tg-18eh">AVX2</td>
    <td class="tg-18eh">x86</td>
    <td class="tg-18eh">AVX2</td>
  </tr>
</thead>
<tbody>
  <tr>
    <td class="tg-73oq">ACE</td>
    <td class="tg-085k">1.54</td>
    <td class="tg-085k">1.33</td>
    <td class="tg-085k">1.64</td>
    <td class="tg-085k">1.01</td>
  </tr>
  <tr>
    <td class="tg-65px">AES</td>
    <td class="tg-65px">-</td>
    <td class="tg-3u1j">1.01</td>
    <td class="tg-65px">-</td>
    <td class="tg-3u1j">1.43</td>
  </tr>
  <tr>
    <td class="tg-73oq">Ascon</td>
    <td class="tg-085k">1.20</td>
    <td class="tg-085k">1.01</td>
    <td class="tg-085k">1.89</td>
    <td class="tg-085k">1.15</td>
  </tr>
  <tr>
    <td class="tg-65px">Chacha20</td>
    <td class="tg-3u1j">1.25</td>
    <td class="tg-3u1j">1.11</td>
    <td class="tg-3u1j">1.23</td>
    <td class="tg-3u1j">1.20</td>
  </tr>
  <tr>
    <td class="tg-73oq">Clyde</td>
    <td class="tg-085k">1.16</td>
    <td class="tg-085k">1.02</td>
    <td class="tg-085k">1.16</td>
    <td class="tg-085k">1.22</td>
  </tr>
  <tr>
    <td class="tg-65px">Gift</td>
    <td class="tg-3u1j">1.69</td>
    <td class="tg-376w">0.93</td>
    <td class="tg-3u1j">1.37</td>
    <td class="tg-3u1j">1.05</td>
  </tr>
  <tr>
    <td class="tg-73oq">Gimli</td>
    <td class="tg-wsl5">0.97</td>
    <td class="tg-wsl5">0.99</td>
    <td class="tg-085k">1.23</td>
    <td class="tg-085k">1.33</td>
  </tr>
  <tr>
    <td class="tg-65px">Pyjamask</td>
    <td class="tg-3u1j">1.35</td>
    <td class="tg-376w">0.99</td>
    <td class="tg-3u1j">1.08</td>
    <td class="tg-3u1j">1.11</td>
  </tr>
  <tr>
    <td class="tg-73oq">Rectangle (H)</td>
    <td class="tg-73oq">-</td>
    <td class="tg-wsl5">0.96</td>
    <td class="tg-73oq">-</td>
    <td class="tg-wsl5">0.97</td>
  </tr>
  <tr>
    <td class="tg-65px">Rectangle (V)</td>
    <td class="tg-65px">1.00</td>
    <td class="tg-376w">0.99</td>
    <td class="tg-376w">0.97</td>
    <td class="tg-376w">0.96</td>
  </tr>
  <tr>
    <td class="tg-73oq">Serpent</td>
    <td class="tg-085k">1.01</td>
    <td class="tg-wsl5">0.99</td>
    <td class="tg-085k">1.27</td>
    <td class="tg-085k">1.27</td>
  </tr>
  <tr>
    <td class="tg-65px">Xoodoo</td>
    <td class="tg-3u1j">1.25</td>
    <td class="tg-376w">0.98</td>
    <td class="tg-3u1j">1.61</td>
    <td class="tg-3u1j">1.39</td>
  </tr>
</tbody>
</table>
</center>

The impact of inlining depends on which C compiler is used, and the
architecture targeted. For instance, inlining every node of Xoodoo
speeds it up by a factor 1.61 on general purpose x86 register when
compiling with gcc, but slows it down by a factor 0.98 on AVX2
registers when compiling with Clang. Overall, inlining every nodes is
generally beneficial for performances, providing speedups of up to
1.89 (Ascon on GP x86 registers with gcc), but can sometimes be
detrimental and reduce the performances by a few percents.

On AVX2 registers, when compiling with Clang, inlining tends to be
slightly detrimental, as can be seen from the lower half of the third
column. Those ciphers are the ones that benefits the less from our
scheduling optimization, as can be seen from the table in the [section
mslicing of the scheduling post]({{site.baseurl}}{% post_url
2020-05-09-scheduling %}#mslicing). 
One of the reason for this performance regression is the fact that
fully inlined AVX code do not use the μop cache (DSB), but falls back
to the MITE. On Gift compiled with Clang for instance, when all nodes
are inlined, almost no μop is issued by the DSB, while when no node is
inlined, 85% of the μops come from the DSB. This translates directly
into a reduces instruction per cycle count: the inlined version is at
2.58 instructions per cycles, while the non-inlined version is at
3.25. The translates in a mere 0.93 slowdown however, because the
fully inlined code still contains 15% less instructions.

The gain of inlining are not only explained by the scheduling that
follows. For instance, scheduling improves the performances of Gift,
Clyde, Xoodoo and Chacha20 on general purpose by merely x1.01, x1.02,
x1.03 and x1.05, yet fully inlining those ciphers speeds them up by
x1.69, x1.16, x1.25 and x1.25 (with Clang). In those cases, both Clang
and gcc chose not to be too aggressive on inlining, probably in order
not to increase code size too much, but this came at the expense of
performances.



Bitslicing, however, sends the inlining heuristics of C compilers much
more astray. A bitsliced node compiles to a C function taking hundreds
of variables as inputs and outputs. For instance, the round function
in DES takes 120 arguments once bitsliced. Calling such a function
requires the caller to push hundreds of variables onto the stack while
the callee has to go through the stack to retrieve them, leading to a
significant execution overhead but also a growth in code
size. Similarly, permutations, depending on how they are written in
the Usuba source, can be compiled into a function which takes hundreds
of arguments and just does assignments, while once inlined, it is
virtually optimized away by copy propagation. C compilers avoid
inlining such functions because their codes are quite large, missing
the fact that they are major bottlenecks.

The following table shows the performance impact of inlining in
bitsliced ciphers:

<center>
<table class="tg" style="undefined;table-layout: fixed; width: 614px; margin-top:30px;margin-bottom:30px">
<colgroup>
<col style="width: 156px">
<col style="width: 115px">
<col style="width: 115px">
<col style="width: 114px">
<col style="width: 114px">
</colgroup>
<thead>
  <tr>
    <td class="tg-mqa1" colspan="5">bitslicing</td>
  </tr>
  <tr>
    <th class="tg-18eh" rowspan="3">Cipher</th>
    <th class="tg-18eh" colspan="4">Inlining speedup</th>
  </tr>
  <tr>
    <td class="tg-18eh" colspan="2">clang</td>
    <td class="tg-18eh" colspan="2">gcc</td>
  </tr>
  <tr>
    <td class="tg-18eh">x86</td>
    <td class="tg-18eh">AVX2</td>
    <td class="tg-18eh">x86</td>
    <td class="tg-18eh">AVX2</td>
  </tr>
</thead>
<tbody>
  <tr>
    <td class="tg-73oq">ACE</td>
    <td class="tg-085k">1.16</td>
    <td class="tg-085k">1.54</td>
    <td class="tg-085k">1.75</td>
    <td class="tg-085k">3.57</td>
  </tr>
  <tr>
    <td class="tg-65px">AES</td>
    <td class="tg-3u1j">1.28</td>
    <td class="tg-3u1j">1.64</td>
    <td class="tg-3u1j">1.27</td>
    <td class="tg-3u1j">1.43</td>
  </tr>
  <tr>
    <td class="tg-73oq">Ascon</td>
    <td class="tg-085k">1.20</td>
    <td class="tg-085k">2.50</td>
    <td class="tg-085k">1.45</td>
    <td class="tg-085k">2.56</td>
  </tr>
  <tr>
    <td class="tg-65px">Clyde</td>
    <td class="tg-3u1j">1.08</td>
    <td class="tg-3u1j">1.79</td>
    <td class="tg-3u1j">1.02</td>
    <td class="tg-3u1j">1.35</td>
  </tr>
  <tr>
    <td class="tg-73oq">DES</td>
    <td class="tg-085k">1.41</td>
    <td class="tg-085k">1.96</td>
    <td class="tg-085k">1.30</td>
    <td class="tg-085k">1.72</td>
  </tr>
  <tr>
    <td class="tg-65px">Gift</td>
    <td class="tg-3u1j">3.70</td>
    <td class="tg-3u1j">5.88</td>
    <td class="tg-3u1j">3.45</td>
    <td class="tg-3u1j">8.33</td>
  </tr>
  <tr>
    <td class="tg-73oq">Gimli</td>
    <td class="tg-085k">1.41</td>
    <td class="tg-085k">2.00</td>
    <td class="tg-085k">1.79</td>
    <td class="tg-085k">3.03</td>
  </tr>
  <tr>
    <td class="tg-65px">Photon</td>
    <td class="tg-3u1j">1.18</td>
    <td class="tg-3u1j">1.75</td>
    <td class="tg-3u1j">2.00</td>
    <td class="tg-3u1j">2.44</td>
  </tr>
  <tr>
    <td class="tg-73oq">Present</td>
    <td class="tg-085k">1.23</td>
    <td class="tg-085k">1.08</td>
    <td class="tg-085k">1.05</td>
    <td class="tg-wsl5">0.97</td>
  </tr>
  <tr>
    <td class="tg-65px">Pyjamask</td>
    <td class="tg-3u1j">5.26</td>
    <td class="tg-3u1j">1.20</td>
    <td class="tg-3u1j">8.33</td>
    <td class="tg-3u1j">8.33</td>
  </tr>
  <tr>
    <td class="tg-73oq">Rectangle</td>
    <td class="tg-085k">1.72</td>
    <td class="tg-085k">2.33</td>
    <td class="tg-085k">1.59</td>
    <td class="tg-085k">2.44</td>
  </tr>
  <tr>
    <td class="tg-65px">Skinny</td>
    <td class="tg-3u1j">2.63</td>
    <td class="tg-3u1j">4.00</td>
    <td class="tg-3u1j">2.78</td>
    <td class="tg-3u1j">4.76</td>
  </tr>
  <tr>
    <td class="tg-73oq">Spongent</td>
    <td class="tg-085k">1.52</td>
    <td class="tg-085k">3.12</td>
    <td class="tg-085k">1.49</td>
    <td class="tg-085k">3.03</td>
  </tr>
  <tr>
    <td class="tg-65px">Subterranean</td>
    <td class="tg-3u1j">2.00</td>
    <td class="tg-3u1j">3.03</td>
    <td class="tg-3u1j">1.96</td>
    <td class="tg-3u1j">2.86</td>
  </tr>
  <tr>
    <td class="tg-73oq">Xoodoo</td>
    <td class="tg-085k">1.37</td>
    <td class="tg-085k">2.33</td>
    <td class="tg-085k">1.47</td>
    <td class="tg-085k">2.08</td>
  </tr>
</tbody>
</table>
</center>

Inlining improves performances in all cases, reaching an impressive 8
times speed up for Pyjamask on general purpose registers with
gcc. With some of those improvements are explained by the scheduling
enabled by inlining, most of them are due to the overhead saved by not
calling functions, and by the copy propagation being able to remove
unnecessary asignments. One of the take away from those benchmarks is
that modern C compilers' heuristic for inlining are not suited for
Usuba-generated bitsliced codes.


### Benchmark-guided optimizations


The impact of some optimizations is hard to predict. Inlining reduces
the overhead of calling functions, but increases the register pressure
and produces codes that do not fully exploit the μop cache (DSB). Our
interleaving algorithm improves instruction level parallelism, but at
the expense of register pressure. Our scheduling algorithm for
bitsliced code tries to reduce register pressure, but this could fail
depending on the heuristic of the C compilers that comes after. Our
scheduling algorithm for msliced codes increases instruction level
parallelism, but this sometimes either increase register pressure or
simply produces codes that the C compiler is less keen to optimize.

Since our goal is to produce the most efficient codes possible, it is
counter-productive that some optimizations hurt the performances. In
order to overcome those "failed" optimizations, Usubac can enable or
disable heuristic optimizations (inlining, scheduling, interleaving)
by benchmarking their outcome. We are able to do this because we are
in a setting where:

 - The compilation time is not as important as with traditional C
   compilers. The generated ciphers will likely be ran for a long
   time, and _need_ to be optimized for performance in order not to
   bottleneck any application. Futhermore, time-costly optimizations
   can be disabled while developping in order to speed up debugging,
   and enabled again to generate the final optimized C code.
   
 - The control flow is independent of the inputs, as a direct
   consequence of using bitslicing and mslicing. While benchmarking a
   C program requires some kind of fuzzy testing, and always risk
   missing a frequent case, in Usuba, every run of a program will have
   the same performances regardless of the inputs.


In the performance results presented throughout above for the
inlining, or in the scheduling and interleaving post, any performance
regression will be caught by Usubac's benchmark engine, and the
optimization will thus be disabled.


The limitation of this technique however, is that benchmarks are run
on the machine used to compile the Usuba program, and the tuning
resulting of the benchmarks may not be optimal for another
architecture. This could be partly mitigated by modifying Usubac's
benchmarking engine to remotely connect to another machine for the
benchmarks, but we have not done so yet.



### Code generation

 - straightforward
 
 - macros (example?)
 
 - easy to change architecture: just change the header

 - no optimizations based on SIMD architecture:
 
   + little to do in SSE/AVX (since generating C code)
   
   + Could be more interesting stuffs on AVX512



### Remark

It would be tempting to integrate the above optimizations as a
domain-specific backend for, say, LLVM [1].  We conjecture that the
resulting compiler would not be radically different nor significantly
faster than Usuba. First, our optimizations are complementary to those
performed by the C compiler (_e.g._, inlining). They would be
expressed almost as-is over LLVM-IR. Second, Table 2 will show that
there is no silver bullet among compilers: we are currently free to
pick the absolute best compiler, even if closed source. Third, going
the extra mile and targeting C simplifies integration in existing code
bases, as witnessed by the incorporation of the C files produced by
HACL* [2] into the Firefox and Wireguard projects. This allows the
cryptographical primitives to potentially outlive the DSL. In the
following, we evaluate the performance of the code generated by our
compiler.


---
## References

[1] C. Lattner, V. Adve, [LLVM: A Compilation Framework for Lifelong Program Analysis & Transformation](https://llvm.org/pubs/2003-09-30-LifelongOptimizationTR.pdf), CGO, 2004.

[2] J.-K. Zinzindohoué _et al_, [HACL*, A Verified Modern Crytographic Library](https://hal.inria.fr/hal-01588421v2/document), ACM Conference on Computer and Communications Security, 2017.
