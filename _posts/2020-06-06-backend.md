---
layout: post
title: Usubac - backend
date: "2020-06-06 00:00:00"
description: The backend of Usuba
lang: en
locale: en_US
author: Darius Mercadier
excerpt: Usubac's backend is responsible of optimizing the Usuba0 code and utlimately generating C code. Masking is also done in the backend, but will be presented in a later post.
comments: true
hidden: false
---


Usubac's backend is responsible of optimizing the Usuba0 code and
utlimately generating C code. Masking is also done in the backend, but
will be presented in a later post.

Generating C code allows us to partially rely on C compiler's
optimizer to improve the performances of the generated
ciphers. However, that alone would not be sufficient to achieve
similar performances as carefuly hand-tuned codes.

We divide our optimizations in two categories. The simple ones (common
subexpression elimination, inlining, unrolling...) are already done by
most C compilers, but we still perform them in Usuba, mainly in order
to improve the effectiveness of the more advanced ones, but also to
not rely too much on the C compiler's optimizers heuristic, which may
not be tailored for cryptographic codes. The more advanced
optimizations include two [scheduling algorithms]({{ site.baseurl}} {%
post_url 2020-05-09-scheduling %}), and an [interleaving pass]({{
site.baseurl}} {% post_url 2020-03-16-interleaving %}), which are
presented in separate posts.

Those advanced optimizations can only be done thanks to the knowledge
we have in Usuba regarding the codes we are dealing with (ciphers),
which C compilers do not have. One of our scheduling algorithm is thus
tailored to reduce the spilling in linear layers of bitsliced ciphers,
while the other one improves instruction level parallelism by mixing
linear layers and S-boxes of msliced ciphers.

Usuba's dataflow programming model is also key for our interleaving
optimization: since Usuba manipulates streams rather than scalars, we
are able to increase instruction level parallelism by processing
several elements of the input stream simultaneously.


### Autotuning


The impact of some optimizations is hard to predict. Inlining reduces
the overhead of calling functions, but increases register pressure and
produces codes that do not fully exploit the Î¼op cache (DSB). Our
interleaving algorithm improves instruction level parallelism, at the
expense of register pressure. Our scheduling algorithm for bitsliced
code tries to reduce register pressure, but sometimes fails to offer
any speedup. Our scheduling algorithm for msliced codes increases
instruction level parallelism, but this sometimes either increase
register pressure or simply produces codes that the C compiler is less
keen to optimize.

Since our goal is to produce the most efficient codes possible, it is
counter-productive that some optimizations reduce performance. In
order to overcome those "failed" optimizations, Usubac's _autotuner_
benchmarks heuristic optimizations (inlining, scheduling,
interleaving), and applies only those which improve performance.

Autotuning is particularly potent in Usuba because we are in a setting
where:

 - The compilation time is not as important as with traditional C
   compilers. The generated ciphers will likely be ran for a long
   time, and _need_ to be optimized for performance in order not to
   bottleneck any application. Futhermore, time-costly optimizations
   can be disabled while prototyping in order to speed up debugging,
   and enabled again to generate the final optimized C code.
   
 - The control flow is independent of the inputs, as a direct
   consequence of using bitslicing and mslicing. While benchmarking a
   generic C program requires a representative workload (at the risk
   of missing a frequent case), every run of a Usuba program will have
   the same performances regardless of its inputs, by design.

However, rather than considering nodes (resp. loops) one by one for
inlining (resp. unrolling), Usubac's autotuner evaluates the impact of
inlining all nodes and unrolling all loops. While this may lead to
sub-optimal performances, the space of all possible combinations of
inlining, unrolling, scheduling and interleave may be too large to be
explorable in reasonable time. This is a know issue of autotuning,
which is solved for instance in Halide [1] by using higher-level
heuristics to guide the search of the autotuner.

We leave for future work to improve Usubac's autotuner to evaluate a
larger space of optimizations. A way to achieve this could be to use
static analysis to prevent some branches to be explored by the
autotuner. For instance, a node with less than 2 instructions will
always be more efficient inline (to remove the overhead of calling a
function). Similarly, instructions per cycle (IPC) can be statically
estimated to guide optimizations: interleaving for instance would have
no way to improve the performance of a code whose IPC is 4. 


At the moment, Usubac's autotuner runs its benchmarks on the machine
used to compile the Usuba program, and the tuning resulting of the
benchmarks may not be optimal for another architecture. In order to
cross-compile for another architecture, the autotuner should either be
disabled, or modified to run (remotely) its benchmarks on another
machine.



### Common Subexpression Elimination, Copy Propagation, Constant Folding

Common subexpression elimination (CSE) is a classical optimization
that aim at preventing identical expressions to be computed multiple
times. When an expression that has already been computed is
recomputed, it is instead replaced by the previously computed
value. For instance,

```c
x = a + b;
y = a + b;
```

would be transformed by CSE into

```c
x = a + b;
y = x;
```

Copy propagation is also a traditional optimization that removes
assignments of a variable into another variable. For instance,

```c
x = a + b;
y = x;
return y;
```

would be transformed by CP into

```c
x = a + b;
return x;
```

Finally, constant folding consists in computing constant expressions
at compile time. The expressions that Usuba simplifies using constant
folding are either arithmetic or bitwise operations between constants
(_eg._ replacing `x = 20 + 22` by `x = 42`) or bitwise operations
whose operand is either `0` or `0xffffffff` (_eg._ replacing `x = a |
0` by `x = a`).

CSE, copy propagation and constant folding are already done by C
compilers. However, performing them in Usubac has two main benefits:

 - It produces smaller C codes, often with very little cost to
   readability. A non-negligible part of the copies removed by copy
   propagation comes from temporary variables introduced by the
   compiler itself. Removing such assignments actually improves
   readability. This matters particularly in bitsliced codes, which
   can contain tens of thousands lines of C code after those
   optimizations, and that would contain hundreds of thousands lines
   without them: on average, those optimizations reduce the number of
   C instructions of the bitsliced ciphers generated by Usuba by
   67%. For instance, ACE bitsliced is about 200.000 instructions
   without those optimizations but only 50.000 with them.
   
 - It makes the [scheduling optimizations]({{ site.baseurl}}{%
   post_url 2020-05-09-scheduling %}) more potent. Needless
   assignments and redundant expressions increase the number of live
   variables, which may throw off the schedulers.

### Loop Unrolling


Unrolling is actually done in the frontend rather than in the
backend. However, it is considered as an optimization in most
compilers, and heavily impacts performances, which is why we discuss
it now.

**Normalization**

In two cases, unrolling is necessary to normalize Usuba code down to
Usuba0. The first case corresponds to shifts and rotations on tuples
that depends on loop variables. For instance,

<div class="language-lustre highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="ow">forall</span> <span class="n">i</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">1</span><span class="o">,</span> <span class="mi">2</span><span class="p">]</span> <span class="p">{</span>
    <span class="p">(</span><span class="n">x0</span><span class="o">,</span><span class="n">x1</span><span class="o">,</span><span class="n">x2</span><span class="p">)</span> <span class="o">:=</span> <span class="p">(</span><span class="n">x0</span><span class="o">,</span><span class="n">x1</span><span class="o">,</span><span class="n">x2</span><span class="p">)</span> <span class="o">&lt;&lt;&lt;</span> <span class="n">i</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div></div>

Since rotations on tuples are resolved at compile time, this one
requires the loop to be unrolled into

```lustre
(x0,x1,x2) := (x0,x1,x2) <<< 1;
(x0,x1,x2) := (x0,x1,x2) <<< 2;
```

Which is then simplified to

```lustre
(x0,x1,x2) := (x1,x2,x0);
(x0,x1,x2) := (x2,x0,x1);
```

Which will be optimized away by copy propagation in the backend.

Unrolling is also needed to normalize to Usuba0 calls to nodes from
arrays of nodes within loops. This is for instance the case with
Serpent, which uses a different S-box for each round, and whose main
loop is thus:

<div class="language-lustre highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="ow">forall</span> <span class="n">i</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">0</span><span class="o">,</span><span class="mi">30</span><span class="p">]</span> <span class="p">{</span>
    <span class="n">state</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">linear_layer</span><span class="p">(</span><span class="n">sbox</span><span class="o">&lt;</span><span class="n">i</span><span class="o">%</span><span class="mi">8</span><span class="o">&gt;</span><span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">^</span> <span class="n">keys</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span>
<span class="p">}</span>
</code></pre></div></div>

Which, after unrolling, becomes:

```lustre
state[1] = linear_layer(sbox0(state[0] ^ keys[0]))
state[2] = linear_layer(sbox1(state[1] ^ keys[1]))
state[3] = linear_layer(sbox2(state[2] ^ keys[2]))
...
```

Note that we chose to exclude both shifts and arrays of nodes from
Usuba0 because they would generate sub-optimal C codes, which would
rely on the C compilers to be willing to optimize away those
constructs. For instance, we could have introduced conditionals in
Usuba in order to normalize the first example (tuple rotation) to

<div class="language-lustre highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="ow">forall</span> <span class="n">i</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">1</span><span class="o">,</span><span class="mi">2</span><span class="p">]</span> <span class="p">{</span>
    <span class="ow">if</span> <span class="p">(</span><span class="n">i</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)</span> <span class="p">{</span>
        <span class="p">(</span><span class="n">x0</span><span class="o">,</span><span class="n">x1</span><span class="o">,</span><span class="n">x2</span><span class="p">)</span> <span class="o">:=</span> <span class="p">(</span><span class="n">x1</span><span class="o">,</span><span class="n">x2</span><span class="o">,</span><span class="n">x0</span><span class="p">);</span>
    <span class="p">}</span> <span class="ow">elsif</span> <span class="p">(</span><span class="n">i</span> <span class="o">==</span> <span class="mi">2</span><span class="p">)</span> <span class="p">{</span>
        <span class="p">(</span><span class="n">x0</span><span class="o">,</span><span class="n">x1</span><span class="o">,</span><span class="n">x2</span><span class="p">)</span> <span class="o">:=</span> <span class="p">(</span><span class="n">x2</span><span class="o">,</span><span class="n">x0</span><span class="o">,</span><span class="n">x1</span><span class="p">);</span>
    <span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div>


And the second example (array of nodes) to

<div class="language-lustre highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="ow">forall</span> <span class="n">i</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">0</span><span class="o">,</span><span class="mi">30</span><span class="p">]</span> <span class="p">{</span>
    <span class="ow">if</span> <span class="p">(</span><span class="n">i</span> <span class="o">%</span> <span class="mi">8</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">state</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">linear_layer</span><span class="p">(</span><span class="n">sbox0</span><span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">^</span> <span class="n">keys</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span>
    <span class="p">}</span> <span class="ow">elsif</span> <span class="p">(</span><span class="n">i</span> <span class="o">%</span> <span class="mi">8</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">state</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">linear_layer</span><span class="p">(</span><span class="n">sbox1</span><span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">^</span> <span class="n">keys</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span>
    <span class="p">}</span> <span class="ow">elsif</span> 
        <span class="o">...</span>
    <span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div>

This Usuba0 could would then have been compiled to C loops. However,
to be efficient, it would have relied on the C compiler unrolling the
loop in order to remove the conditionals and optimize (_e.g._ with
copy propagation) the resulting code. In practice, C compilers avoid
unrolling large loop, and performing the unrolling in Usuba leads to
better and more predictable performances.


**Optimization**

In practice, Usubac automatically unrolls all loops by
default. Experimentally, this produce the most efficient codes. The
user can still use the flag `-no-unroll` to disable non-essential
unrolling (_i.e._ unrolling that is not required by the
normalization). In the following, we explain the reasoning behind the
aggressive unrolling performed by Usubac.

Loop unrolling is clearly beneficial for bitsliced ciphers as almost
all ciphers contain some kind of permutations, shifts or rotations to
implement their linear layers. After unrolling (and only in bitslice
mode), those operations can be optimized away by copy propagation at
compile time.

For msliced codes, the rational for unrolling is more subtle. Very
small loops are always more efficient when unrolled since the overhead
of looping would hurt performance.

Furthermore, unrolling small and medium-sized loops is often
beneficial as well because it allows our scheduling algorithm to be
more efficient. Most loops contain dependencies from one iteration to
the next one, which may limit their performances, and thus benefit
from being unrolled and interleaved (by the scheduler) with other
parts of the cipher. The [scheduling post]({{ site.baseurl}}{% post_url 2020-05-09-scheduling %}#mslicing) 
shows for instance the example of ACE, which is bottlenecked by a loop
and that our scheduling algorithm is able to optimize by interleaving
3 loops (at the condition that they are unrolled).


We may want to keep large loops in the final code in order to reduce
code size and maximize the usage of the Î¼op cache (DSB). For instance,
in Chacha20, unrolling the main loop (_i.e._ the one that calls the
round function) does not offer any performance improvement (nor does
it decrease performance for that matter). However, it is hard to chose
an upper bound above which unrolling should not be done. For instance,
Pyjamask contains a matrix multiplication in a loop:

<div class="language-lustre highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="ow">forall</span> <span class="n">i</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">0</span><span class="o">,</span> <span class="mi">3</span><span class="p">]</span> <span class="p">{</span>
    <span class="n">output</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">mat_mult</span><span class="p">(</span><span class="n">M</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">,</span> <span class="n">input</span><span class="p">[</span><span class="n">i</span><span class="p">]);</span>
<span class="p">}</span>
</code></pre></div></div>


After inlining, `mat_mult` becomes 160 instructions, which is more
that the number of instructions in Rectangle's round (15), or in
Ascon's (32) or in Chacha20's (96). However, those instructions are
heavily bottlenecked by data-dependencies, and unrolling the loop in
Usuba (Clang choses not to unroll it on its own) speeds up the
performances of Pyjasmask by a factor 1.68.


In practice, we thus chose to aggressively unroll all loops in Usuba,
regardless of their contents. We never observed any performance
regression because of our unrolling.


### Inlining

The decision of inlining nodes is partly justified by the usual
reasoning applied by C compilers: a function call implies a
significant overhead that, for very frequently called functions (such
as S-boxes, in our case) compensates for the increase in code
size. However, Usuba's [m-sliched code scheduling algorithm]({{
site.baseurl}}{% post_url 2020-05-09-scheduling %}) tries to
interleave nodes to increase instruction level parallelism, and thus
requires them to be inlined. We thus perform some inlining in
Usuba. The following table shows the speedups gained by inlining all
nodes on some msliced ciphers, compared to inlining none:

<style type="text/css">
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  overflow:hidden;padding:10px 5px;word-break:normal;}
.tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}
.tg .tg-18eh{border-color:#000000;font-weight:bold;text-align:center;vertical-align:middle}
.tg .tg-65px{background-color:#ecf4ff;border-color:#000000;text-align:left;vertical-align:top}
.tg .tg-wsl5{border-color:#000000;color:#fe0000;text-align:left;vertical-align:top}
.tg .tg-3u1j{background-color:#ecf4ff;border-color:#000000;color:#32cb00;text-align:left;vertical-align:top}
.tg .tg-mqa1{border-color:#000000;font-weight:bold;text-align:center;vertical-align:top}
.tg .tg-73oq{border-color:#000000;text-align:left;vertical-align:top}
.tg .tg-085k{border-color:#000000;color:#32cb00;text-align:left;vertical-align:top}
.tg .tg-376w{background-color:#ecf4ff;border-color:#000000;color:#fe0000;text-align:left;vertical-align:top}
</style>

<center>
<table class="tg" style="undefined;table-layout: fixed; width: 614px; margin-top:30px;margin-bottom:30px">
<colgroup>
<col style="width: 156px">
<col style="width: 115px">
<col style="width: 115px">
<col style="width: 114px">
<col style="width: 114px">
</colgroup>
<thead>
  <tr>
    <td class="tg-mqa1" colspan="5">mslicing</td>
  </tr>
  <tr>
    <th class="tg-18eh" rowspan="3">Cipher</th>
    <th class="tg-18eh" colspan="4">Inlining speedup</th>
  </tr>
  <tr>
    <td class="tg-18eh" colspan="2">clang</td>
    <td class="tg-18eh" colspan="2">gcc</td>
  </tr>
  <tr>
    <td class="tg-18eh">x86</td>
    <td class="tg-18eh">AVX2</td>
    <td class="tg-18eh">x86</td>
    <td class="tg-18eh">AVX2</td>
  </tr>
</thead>
<tbody>
  <tr>
    <td class="tg-73oq">ACE</td>
    <td class="tg-085k">1.54</td>
    <td class="tg-085k">1.33</td>
    <td class="tg-085k">1.64</td>
    <td class="tg-085k">1.01</td>
  </tr>
  <tr>
    <td class="tg-65px">AES</td>
    <td class="tg-65px">-</td>
    <td class="tg-3u1j">1.01</td>
    <td class="tg-65px">-</td>
    <td class="tg-3u1j">1.43</td>
  </tr>
  <tr>
    <td class="tg-73oq">Ascon</td>
    <td class="tg-085k">1.20</td>
    <td class="tg-085k">1.01</td>
    <td class="tg-085k">1.89</td>
    <td class="tg-085k">1.15</td>
  </tr>
  <tr>
    <td class="tg-65px">Chacha20</td>
    <td class="tg-3u1j">1.25</td>
    <td class="tg-3u1j">1.11</td>
    <td class="tg-3u1j">1.23</td>
    <td class="tg-3u1j">1.20</td>
  </tr>
  <tr>
    <td class="tg-73oq">Clyde</td>
    <td class="tg-085k">1.16</td>
    <td class="tg-085k">1.02</td>
    <td class="tg-085k">1.16</td>
    <td class="tg-085k">1.22</td>
  </tr>
  <tr>
    <td class="tg-65px">Gift</td>
    <td class="tg-3u1j">1.69</td>
    <td class="tg-376w">0.93</td>
    <td class="tg-3u1j">1.37</td>
    <td class="tg-3u1j">1.05</td>
  </tr>
  <tr>
    <td class="tg-73oq">Gimli</td>
    <td class="tg-wsl5">0.97</td>
    <td class="tg-wsl5">0.99</td>
    <td class="tg-085k">1.23</td>
    <td class="tg-085k">1.33</td>
  </tr>
  <tr>
    <td class="tg-65px">Pyjamask</td>
    <td class="tg-3u1j">1.35</td>
    <td class="tg-376w">0.99</td>
    <td class="tg-3u1j">1.08</td>
    <td class="tg-3u1j">1.11</td>
  </tr>
  <tr>
    <td class="tg-73oq">Rectangle (H)</td>
    <td class="tg-73oq">-</td>
    <td class="tg-wsl5">0.96</td>
    <td class="tg-73oq">-</td>
    <td class="tg-wsl5">0.97</td>
  </tr>
  <tr>
    <td class="tg-65px">Rectangle (V)</td>
    <td class="tg-65px">1.00</td>
    <td class="tg-376w">0.99</td>
    <td class="tg-376w">0.97</td>
    <td class="tg-376w">0.96</td>
  </tr>
  <tr>
    <td class="tg-73oq">Serpent</td>
    <td class="tg-085k">1.01</td>
    <td class="tg-wsl5">0.99</td>
    <td class="tg-085k">1.27</td>
    <td class="tg-085k">1.27</td>
  </tr>
  <tr>
    <td class="tg-65px">Xoodoo</td>
    <td class="tg-3u1j">1.25</td>
    <td class="tg-376w">0.98</td>
    <td class="tg-3u1j">1.61</td>
    <td class="tg-3u1j">1.39</td>
  </tr>
</tbody>
</table>
</center>

The impact of inlining depends on which C compiler is used, and the
architecture targeted. For instance, inlining every node of Xoodoo
speeds it up by a factor 1.61 on general purpose x86 register when
compiling with gcc, but slows it down by a factor 0.98 on AVX2
registers when compiling with Clang. Overall, inlining every nodes is
generally beneficial for performances, providing speedups of up to
1.89 (Ascon on GP x86 registers with gcc), but can sometimes be
detrimental and reduce the performances by a few percents.

On AVX2 registers, when compiling with Clang, inlining tends to be
slightly detrimental, as can be seen from the lower half of the third
column. Those ciphers are the ones that benefits the less from our
scheduling optimization, as can be seen from the table in the [section
mslicing of the scheduling post]({{site.baseurl}}{% post_url
2020-05-09-scheduling %}#mslicing).  One of the reason for this
performance regression is the fact that fully inlined AVX code do not
use the Î¼op cache (DSB), but falls back to the MITE. On Gift compiled
with Clang for instance, when all nodes are inlined, almost no Î¼op is
issued by the DSB, while when no node is inlined, 85% of the Î¼ops come
from the DSB. This translates directly into a reduces instruction per
cycle count: the inlined version is at 2.58 instructions per cycles,
while the non-inlined version is at 3.25. The translates in a mere
0.93 slowdown however, because the fully inlined code still contains
15% less instructions. This impacts less general purpose registers
than AVX because their instructions are smaller, and the MITE can thus
decode more of them each cycle.

The gain of inlining are not only explained by the scheduling that
follows. For instance, scheduling improves the performances of Gift,
Clyde, Xoodoo and Chacha20 on general purpose by merely x1.01, x1.02,
x1.03 and x1.05, yet fully inlining those ciphers speeds them up by
x1.69, x1.16, x1.25 and x1.25 (with Clang). In those cases, both Clang
and gcc chose not to be too aggressive on inlining, probably in order
not to increase code size too much, but this came at the expense of
performance.


Bitslicing, however, definitelly confuses the inlining heuristics of C
compilers. A bitsliced node compiles to a C function taking hundreds
of variables as inputs and outputs. For instance, the round function
in DES takes 120 arguments once bitsliced. Calling such a function
requires the caller to push hundreds of variables onto the stack while
the callee has to go through the stack to retrieve them, leading to a
significant execution overhead but also a growth in code
size. Similarly, a permutation may be compiled into a function that
takes hundreds of arguments and just does assignments, while once
inlined, it is virtually optimized away by copy propagation. C
compilers avoid inlining such functions because their code is quite
large, missing the fact that they would be optimized away.

The following table shows the performance impact of inlining in
bitsliced ciphers:

<center>
<table class="tg" style="undefined;table-layout: fixed; width: 614px; margin-top:30px;margin-bottom:30px">
<colgroup>
<col style="width: 156px">
<col style="width: 115px">
<col style="width: 115px">
<col style="width: 114px">
<col style="width: 114px">
</colgroup>
<thead>
  <tr>
    <td class="tg-mqa1" colspan="5">bitslicing</td>
  </tr>
  <tr>
    <th class="tg-18eh" rowspan="3">Cipher</th>
    <th class="tg-18eh" colspan="4">Inlining speedup</th>
  </tr>
  <tr>
    <td class="tg-18eh" colspan="2">clang</td>
    <td class="tg-18eh" colspan="2">gcc</td>
  </tr>
  <tr>
    <td class="tg-18eh">x86</td>
    <td class="tg-18eh">AVX2</td>
    <td class="tg-18eh">x86</td>
    <td class="tg-18eh">AVX2</td>
  </tr>
</thead>
<tbody>
  <tr>
    <td class="tg-73oq">ACE</td>
    <td class="tg-085k">1.16</td>
    <td class="tg-085k">1.54</td>
    <td class="tg-085k">1.75</td>
    <td class="tg-085k">3.57</td>
  </tr>
  <tr>
    <td class="tg-65px">AES</td>
    <td class="tg-3u1j">1.28</td>
    <td class="tg-3u1j">1.64</td>
    <td class="tg-3u1j">1.27</td>
    <td class="tg-3u1j">1.43</td>
  </tr>
  <tr>
    <td class="tg-73oq">Ascon</td>
    <td class="tg-085k">1.20</td>
    <td class="tg-085k">2.50</td>
    <td class="tg-085k">1.45</td>
    <td class="tg-085k">2.56</td>
  </tr>
  <tr>
    <td class="tg-65px">Clyde</td>
    <td class="tg-3u1j">1.08</td>
    <td class="tg-3u1j">1.79</td>
    <td class="tg-3u1j">1.02</td>
    <td class="tg-3u1j">1.35</td>
  </tr>
  <tr>
    <td class="tg-73oq">DES</td>
    <td class="tg-085k">1.41</td>
    <td class="tg-085k">1.96</td>
    <td class="tg-085k">1.30</td>
    <td class="tg-085k">1.72</td>
  </tr>
  <tr>
    <td class="tg-65px">Gift</td>
    <td class="tg-3u1j">3.70</td>
    <td class="tg-3u1j">5.88</td>
    <td class="tg-3u1j">3.45</td>
    <td class="tg-3u1j">8.33</td>
  </tr>
  <tr>
    <td class="tg-73oq">Gimli</td>
    <td class="tg-085k">1.41</td>
    <td class="tg-085k">2.00</td>
    <td class="tg-085k">1.79</td>
    <td class="tg-085k">3.03</td>
  </tr>
  <tr>
    <td class="tg-65px">Photon</td>
    <td class="tg-3u1j">1.18</td>
    <td class="tg-3u1j">1.75</td>
    <td class="tg-3u1j">2.00</td>
    <td class="tg-3u1j">2.44</td>
  </tr>
  <tr>
    <td class="tg-73oq">Present</td>
    <td class="tg-085k">1.23</td>
    <td class="tg-085k">1.08</td>
    <td class="tg-085k">1.05</td>
    <td class="tg-wsl5">0.97</td>
  </tr>
  <tr>
    <td class="tg-65px">Pyjamask</td>
    <td class="tg-3u1j">5.26</td>
    <td class="tg-3u1j">1.20</td>
    <td class="tg-3u1j">8.33</td>
    <td class="tg-3u1j">8.33</td>
  </tr>
  <tr>
    <td class="tg-73oq">Rectangle</td>
    <td class="tg-085k">1.72</td>
    <td class="tg-085k">2.33</td>
    <td class="tg-085k">1.59</td>
    <td class="tg-085k">2.44</td>
  </tr>
  <tr>
    <td class="tg-65px">Skinny</td>
    <td class="tg-3u1j">2.63</td>
    <td class="tg-3u1j">4.00</td>
    <td class="tg-3u1j">2.78</td>
    <td class="tg-3u1j">4.76</td>
  </tr>
  <tr>
    <td class="tg-73oq">Spongent</td>
    <td class="tg-085k">1.52</td>
    <td class="tg-085k">3.12</td>
    <td class="tg-085k">1.49</td>
    <td class="tg-085k">3.03</td>
  </tr>
  <tr>
    <td class="tg-65px">Subterranean</td>
    <td class="tg-3u1j">2.00</td>
    <td class="tg-3u1j">3.03</td>
    <td class="tg-3u1j">1.96</td>
    <td class="tg-3u1j">2.86</td>
  </tr>
  <tr>
    <td class="tg-73oq">Xoodoo</td>
    <td class="tg-085k">1.37</td>
    <td class="tg-085k">2.33</td>
    <td class="tg-085k">1.47</td>
    <td class="tg-085k">2.08</td>
  </tr>
</tbody>
</table>
</center>

Inlining improves performances in all cases, reaching an impressive 8
times speed up for Pyjamask on general purpose registers with
gcc. While some of those improvements are explained by the scheduling
opportunities enabled by inlining, most of them are due to the
overhead saved by not calling functions, and by the copy propagation
being able to remove unnecessary asignments. One of the take away from
those benchmarks is that C compilers' heuristic for inlining are not
suited for Usuba-generated bitsliced codes.



### Code generation


Compiling Usuba0 to C is very straightforward. Nodes are translated to
function definitions, and node calls to function calls. All
expressions in Usuba0 have a natural equivalent in C, with the
exception of Usuba's `Shuffle`, which can only be compiled for SIMD
architecture and uses the available intrinsics (`_mm256_shuffle_epi32`
or `_mm256_shuffle_epi8` on AVX2 for instance).

The generated C code relies on macros rather than inline
operators. For instance, compiling the following Usuba0 nodes to C:

<div class="language-lustre highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">node</span> <span class="nf">sbox</span> <span class="p">(</span><span class="n">i0</span><span class="o">:</span><span class="n">u32</span><span class="o">,</span> <span class="n">i1</span><span class="o">:</span><span class="n">u32</span><span class="p">)</span>
     <span class="k">returns</span> <span class="p">(</span><span class="n">r0</span><span class="o">:</span><span class="n">u32</span><span class="o">,</span> <span class="n">r1</span><span class="o">:</span><span class="n">u32</span><span class="p">)</span>
<span class="k">vars</span> <span class="n">t1</span> <span class="o">:</span> <span class="n">u32</span>
<span class="k">let</span>
    <span class="n">t1</span> <span class="o">=</span> <span class="o">~</span><span class="n">i0</span><span class="p">;</span>
    <span class="n">r0</span> <span class="o">=</span> <span class="n">t1</span> <span class="o">&amp;</span> <span class="n">i1</span><span class="p">;</span>
    <span class="n">r1</span> <span class="o">=</span> <span class="n">t1</span> <span class="o">|</span> <span class="n">i1</span><span class="p">;</span>
<span class="k">tel</span>
</code></pre></div></div>

produces


<div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">void</span> <span class="nf">sbox__</span> <span class="p">(</span><span class="cm">/*inputs*/</span> <span class="kt">DATATYPE</span> <span class="n">i0__</span><span class="p">,</span><span class="kt">DATATYPE</span> <span class="n">i1__</span><span class="p">,</span> 
             <span class="cm">/*outputs*/</span> <span class="kt">DATATYPE</span><span class="o">*</span> <span class="n">r0__</span><span class="p">,</span><span class="kt">DATATYPE</span><span class="o">*</span> <span class="n">r1__</span><span class="p">)</span> <span class="p">{</span>
  <span class="c1">// Variables declaration</span>
  <span class="kt">DATATYPE</span> <span class="n">t1__</span><span class="p">;</span>

  <span class="c1">// Instructions (body)</span>
  <span class="n">t1__</span> <span class="o">=</span> <span class="n">NOT</span><span class="p">(</span><span class="n">i0__</span><span class="p">);</span>
  <span class="o">*</span><span class="n">r0__</span> <span class="o">=</span> <span class="n">AND</span><span class="p">(</span><span class="n">t1__</span><span class="p">,</span><span class="n">i1__</span><span class="p">);</span>
  <span class="o">*</span><span class="n">r1__</span> <span class="o">=</span> <span class="n">OR</span><span class="p">(</span><span class="n">t1__</span><span class="p">,</span><span class="n">i1__</span><span class="p">);</span>
<span class="p">}</span>
</code></pre></div></div>


Where `DATATYPE` is `unsigned int` on 32-bit registers, `__m128i` on
SSE, `__m256i` on AVX2, etc., and `NOT`, `AND` and `OR` are defined to
use the architecture's instructions. Using macros allows us to change
the architecture of the generated code by simply changing a
header. The new header must provide the same instructions, which means
that for instance a code compiled for AVX2 and using `Shuffle`s cannot
be ran on general purpose registers by simply changing the header
since no shuffle would be available.


Usubac performs no architecture-specific optimizations, beyond its
scheduling and interleaving that targets superscalar
architectures. Put otherwise, we do not compile any differently a code
for SSE or AVX2, execpt that Usubac's automatic benchmarking may
select different optimizations for each architecture. For general
purpose registers, SSE, AVX and AVX2, the instructions used for
cryptographic primitives are fairly similar, and we felt that there
was no need to optimize differently on each architecture. In most
cases where architecture-specific instructions can be used to speed up
computation, Clang and gcc are able to detect it and perform the
optimization for us.


The AVX512 instruction is however much richer, and opens the door for
more optimizations. For instance, it offers the instruction
`vpternlogd` which can compute any boolean function with 3 inputs. It
can be useful to speed up S-boxes in particular. For instance,

```c
t1 = a ^ b;
t2 = t1 & c;
```

Can be written with a single `vpternlog` as

<div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">t2</span> <span class="o">=</span> <span class="n">_mm512_ternarylogic_epi64</span><span class="p">(</span><span class="n">a</span><span class="p">,</span><span class="n">b</span><span class="p">,</span><span class="n">c</span><span class="p">,</span><span class="mi">0b00010100</span><span class="p">);</span>
</code></pre></div></div>

Thus requiring one instruction rather than two. Clang is able to
automatically perform this optimization in some cases, but we leave
for future work to evaluate whether Usubac could improve on Clang on
that aspect.


---
## References

[1] J Ragan-Kelley _et al._, [Halide: a language and compiler for optimizing parallelism, locality, and recomputation in image processing pipelines](https://dl.acm.org/doi/pdf/10.1145/2499370.2462176), PLDI, 2013.
