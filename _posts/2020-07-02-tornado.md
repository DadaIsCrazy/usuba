---
layout: post
title: Tornado, Automatic Generation of Probing-Secure Masked Sliced Implementations
date: "2020-07-02 00:00:00"
description: 
lang: en
locale: en_US
author: Darius Mercadier
excerpt: 
comments: false
hidden: true
---

Given a high-level description of a cryptographic primitive, Tornado
synthesizes a masked implementation using ISW-based multiplication and
refresh gadgets [1], and sharewise addition gadgets. The key role of
Usuba is to automate the generation of a sliced implementation, upon
which tightPROVE+ is then able to verify either the bit probing or
register probing security, or identify the necessary refreshes. By
integrating both tools, we derive a probing-secure masked
implementation from a high-level description of a cipher.

## Tornado's architecture

The overall architecture of Tornado is shown below:

<p align="center" style="margin-top:30px;margin-bottom:30px">
<img src="{{ site.baseurl }}/assets/images/blog/tornado-pipeline-final-small.png">
</p>

After normalization and optimization, the Usuba0 program is sent to
tightPROVE<sup>+</sup>, which adds refreshes if necessary. The output
of tightPROVE<sup>+</sup> is translated back to Usuba0, which Usubac
then masks: variables are replaced with arrays of shares, linear
operators and non-linear operators involving constants are applied
share-wise, and other non-linear operators and refreshes are left as
is to be replaced by masked gadgets during C code generation. Before
generating C code, a pass of _loop fusion_ merges share-wise
applications of linear operators when possible.


### Encoding and Decoding Usuba0 for tightPROVE<sup>+</sup>

Tornado targets low-end embedded devices with drastic memory
constraints. It is therefore essential to produce a C code that uses
loops and functions, in order to reduce code size, especially when
masking at high orders. This is one of the main motivation to keep
loops and nodes in the Usuba pipeline despite their semantics being
defined through macro-expansion to simpler constructions.

However, the input language of tightPROVE+ consists of unrolled
inlined register-based circuits. Usubac thus fully unrolls and inlines
the Usuba0 code before generating tightPROVE<sup>+</sup>'s input, but
the refreshes inserted by tightPROVE<sup>+</sup> are propagated back
into the Usuba0 program with loops, nodes and node calls.

To do so, when inlining and unrolling, we keep track of the origin of
each instruction: which node they come from, and which instruction in
that node. For each refresh inserted by TightPROVE<sup>+</sup>, we
therefore know where (_i.e._ which node and loop if any) the refreshed
variable comes from, and are therefore able to insert the refresh
directly in the right node.

In order to insert the refreshes as early as possible, a first pass
sends each node to tightPROVE<sup>+</sup> for verification. Only then
are all nodes inlined and the whole program sent to
tightPROVE<sup>+</sup>. This is useful for instance on Ace, which
contains the following node:

```lustre
node f(x:u32) returns (y:u32)
let
    y = ((x <<< 5) & x) ^ (x <<< 1)
tel
```

This node is obviously vulnerable to probing attacks, and
tightPROVE<sup>+</sup> is able to insert a refresh on one of the `x`
to make it probing-secure. Once the secure version of `f` is inlined
in Ace, no other refreshes are needed.

Since tightPROVE<sup>+</sup>'s verification often takes hours or even
days to verify a whole cipher [3], Usubac uses a cache to avoid
calling tightPROVE<sup>+</sup> multiple times on the same
circuit. This is useful especially when debugging a cipher written in
Usuba, or when compiling several ciphers that share some nodes.

<!--

```python
def refresh_prog(ua_source):
   ua_expanded  = inline_and_unroll(ua_source)
   tp_source    = usuba_to_tightprove(ua_expanded)
   tp_refreshed = call_tightprove(tp_source)
   ua_refreshed = tightprove_to_usuba(tp_refreshed, ua_expanded)
   return propagate_refreshes(ua_refreshed, ua_source)
```
-->


### Masking

To mask an Usuba0 program, Usubac replaces each variable with an array
of shares, and each operator with a masked gadget. The gadget to mask
a linear operator (`xor`) is simply a loop applying the operator on
each share, written directly in Usuba. To mask a negation, we only
negate the first share (since `~(r0 ^ r1 ^ ... ^ rk) == ~r0 ^ r1 ^
... ^ rk`), still in Usuba. To mask non-linear operators (`and` and
`or`), however, we introduce the operators `m&` and `m|` into Usuba,
which are transformed into calls to masked C gadgets when generating
the C code. In particular, for the multiplication (`and`), we use the
so-called ISW gadget gadget introduced in [1]:


```c
static void isw_mult(uint32_t *res,
                     const uint32_t *op1,
                     const uint32_t *op2) {
    for (int i = 0; i <= MASKING_ORDER; i++)
        res[i] = 0;
     
    for (int i = 0; i <= MASKING_ORDER; i++) {
        res[i] ^= op1[i] & op2[i];
        
        for (int j = i+1; j <= MASKING_ORDER; j++) {
            uint32_t rnd = get_random();
            res[i] ^= rnd;
            res[j] ^= (rnd ^ (op1[i] & op2[j])) ^ (op1[j] & op2[i]);
        }
    }
}
```

while the `or`s are transformed into `not` and `and`, since `a | b ==
~(~a & ~b)`. Similarly, refreshes, either inserted manually by the
developper or automatically by tightPROVE<sup>+</sup>, are compiled
into calls to the ISW refresh routine when generating C code:


```c
static void isw_refresh(uint32_t *res,
                        const uint32_t *in) {
    for (int i=0; i<=MASKING_ORDER; i++)
        res[i] = in[i];

    for (int i=0; i<=MASKING_ORDER; i++) {
        for (int j=i+1; j<=MASKING_ORDER; j++) {
            uint32_t rnd = get_random();
            res[i] ^= rnd;
            res[j] ^= rnd;
        }
    }
}
```


#### Constants

Constants are not secret values and thus do not need to be
shared. Furthermore, when multiplying a constant with a secret value,
there is no need to use the ISW mutliplication gadgets: we can simply
multiply each share of the secret value with the constant. The cost of
masking a multiplication by a constant is thus linear in the number of
shares, rather than quadratic as the ISW gadget is. Our benchmarks
(see section "Evaluation" below) show that indeed, the more masked
multiplication a cipher has, the slower it is. Avoiding unnecessary
masked multiplications is thus essential.

For instance, Pyjamask's linear layer contains a matrix
multiplication, implemented by calling 4 times the following Usuba
node, which multiplies two 32-bit vectors `a` and `b`:

<div class="language-lustre highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">node</span> <span class="nf">col_mult</span> <span class="p">(</span><span class="n">a</span><span class="o">:</span><span class="n">b32</span><span class="o">,</span><span class="n">b</span><span class="o">:</span><span class="n">b32</span><span class="p">)</span> <span class="k">returns</span> <span class="p">(</span><span class="n">r</span><span class="o">:</span><span class="n">b1</span><span class="p">)</span>
<span class="k">vars</span> <span class="n">acc</span><span class="o">:</span><span class="n">b32</span>
<span class="k">let</span>
    <span class="n">acc</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">a</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&amp;</span> <span class="n">b</span><span class="p">[</span><span class="mi">0</span><span class="p">];</span>
    <span class="k">forall</span> <span class="n">i</span> <span class="k">in</span> <span class="p">[</span><span class="mi">1</span><span class="o">,</span><span class="mi">31</span><span class="p">]</span> <span class="p">{</span>
        <span class="n">acc</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">acc</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">^</span> <span class="p">(</span><span class="n">a</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">&amp;</span> <span class="n">b</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
    <span class="p">}</span>
    <span class="n">r</span> <span class="o">=</span> <span class="n">acc</span><span class="p">[</span><span class="mi">31</span><span class="p">]</span>
<span class="k">tel</span>
</code></pre></div></div>

When this node is called in Pyjamask, `b` is always constant, and the
multiplication (`a[i] & b[i]`) therefore does not need to be
masked. Usubac uses a simple inference algorithm to track which
variables are constant and which are not, and is thus able to identify
that `b` does not need to be shared, and that the multiplication `a[i]
& b[i]` does not need to be masked.  This optimization both reduces
stack usage (since constant variables are kept as a single share
rather than an array of share) and increases performances (since
multiplying by a constant becomes linear rather than quadratic).


### Loop Fusion

Since each linear operation is replaced with a loop applying the
operation on each share, the masked code contains a lot of loops. The
overhead of those loops becomes quickly detrimental to
performances. Consider for instance the following Usuba0 snippet:

```lustre
x = a ^ b;
y = c ^ d;
z = x ^ y;
```

After masking, it becomes:


<div class="language-lustre highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">forall</span> <span class="n">i</span> <span class="k">in</span> <span class="p">[</span><span class="mi">0</span><span class="o">,</span> <span class="n">MASKING_ORDER</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="p">{</span>
    <span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">a</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">^</span> <span class="n">b</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
<span class="p">}</span>
<span class="k">forall</span> <span class="n">i</span> <span class="k">in</span> <span class="p">[</span><span class="mi">0</span><span class="o">,</span> <span class="n">MASKING_ORDER</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="p">{</span>
    <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">c</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">^</span> <span class="n">d</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
<span class="p">}</span>
<span class="k">forall</span> <span class="n">i</span> <span class="k">in</span> <span class="p">[</span><span class="mi">0</span><span class="o">,</span> <span class="n">MASKING_ORDER</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="p">{</span>
    <span class="n">z</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">^</span> <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
<span class="p">}</span>
</code></pre></div></div>


In order to reduce the overhead of looping over each share for linear
operations, we aggressively perform loop fusion (also called _loop
jamming_) in Usubac, which consists in replacing multiple loops with a
single one. The above snippet is thus optimized to:


<div class="language-lustre highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">forall</span> <span class="n">i</span> <span class="k">in</span> <span class="p">[</span><span class="mi">0</span><span class="o">,</span> <span class="n">MASKING_ORDER</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="p">{</span>
    <span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">a</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">^</span> <span class="n">b</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
    <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">c</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">^</span> <span class="n">d</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
    <span class="n">z</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">^</span> <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
<span class="p">}</span>
</code></pre></div></div>


Loop fusion is also able to reduce the amount of stack used by
allowing Usubac to replace temporary arrays with temporary
variables. In the example above, Usubac would thus convert `x` and `y`
into scalars rather than arrays and produce the following code:


<div class="language-lustre highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">forall</span> <span class="n">i</span> <span class="k">in</span> <span class="p">[</span><span class="mi">0</span><span class="o">,</span> <span class="n">MASKING_ORDER</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="p">{</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">a</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">^</span> <span class="n">b</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">c</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">^</span> <span class="n">d</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
    <span class="n">z</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span> <span class="o">^</span> <span class="n">y</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div></div>


On embedded devices (which Tornado targets), this is especially
beneficial since the amount of stack available is very limited (_e.g._
a few dozen of kilobytes).

C compilers also perform loop fusion, but experimentally, they are
less aggressive than Usubac. We thus obtain better performances by
fusing loops directly in Usubac. On the 11 ciphers of the NIST
competition we implemented in Usuba and compiled with Tornado,
performing loop fusion in Usubac allows us to reduce stack usage of
our bitsliced implementations by 11kB on average whereas this saves
us, on average, 3kB of stack for our _n_-sliced implementations
(note that our platform offers a measly 96kB of SRAM). It also
positively impacts performance, with a 16% average speedup for
bitslicing and a 21% average speedup for n-slicing.


## Evaluation

We used Tornado to compare 11 cryptographic primitives from the second
round of the NIST lightweight cryptography competition. The choice of
cryptographic primitives was made on the basis that they were
self-identified as being amenable to masking. We stress that we do not
focus on the full authenticated encryption, message authentication, or
hash protocols but on the underlying primitives, mostly block ciphers
and permutations. The complete list of primitives follows:


<style type="text/css">
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  overflow:hidden;padding:10px 5px;word-break:normal;}
.tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}
.tg .tg-9wq8{border-color:inherit;text-align:center;vertical-align:middle}
.tg .tg-c3ow{border-color:inherit;text-align:center;vertical-align:top}
.tg .tg-uzvj{border-color:inherit;font-weight:bold;text-align:center;vertical-align:middle}
.tg .tg-xwyw{border-color:#000000;text-align:center;vertical-align:middle}
.tg .tg-wruy{border-color:#000000;color:#32cb00;text-align:center;vertical-align:middle}
.tg .tg-73t7{border-color:#000000;color:#cb0000;text-align:center;vertical-align:middle}
.tg .tg-y0n7{background-color:#efefef;text-align:center;vertical-align:middle}
</style>
<center>
<table class="tg" style="margin-bottom:25px">
<thead>
  <tr>
    <th class="tg-uzvj">submissions</th>
    <th class="tg-uzvj">primitive</th>
    <th class="tg-uzvj">n-sliceable</th>
    <th class="tg-uzvj">slice size</th>
  </tr>
</thead>
<tbody>
  <tr>
    <td class="tg-y0n7" colspan="4">block ciphers</td>
  </tr>
  <tr>
    <td class="tg-9wq8">Gift-Cofb [7],<br>Hyena [17],<br>Sundae-Gift [18]</td>
    <td class="tg-9wq8">Gift-128</td>
    <td class="tg-wruy">✔</td>
    <td class="tg-xwyw">32</td>
  </tr>
  <tr>
    <td class="tg-c3ow">Pyjamask [10]</td>
    <td class="tg-9wq8">Pyjamask-128</td>
    <td class="tg-wruy">✔</td>
    <td class="tg-xwyw">32</td>
  </tr>
  <tr>
    <td class="tg-9wq8">Skinny [11],<br>Romulus [19]</td>
    <td class="tg-9wq8">Skinny-128-256</td>
    <td class="tg-73t7">✖</td>
    <td class="tg-xwyw">-</td>
  </tr>
  <tr>
    <td class="tg-9wq8">Spook [6]</td>
    <td class="tg-9wq8">Clyde-128</td>
    <td class="tg-wruy">✔</td>
    <td class="tg-xwyw">32</td>
  </tr>
  <tr>
    <td class="tg-y0n7" colspan="4">permutations</td>
  </tr>
  <tr>
    <td class="tg-9wq8">Ace [4]</td>
    <td class="tg-9wq8">Ace</td>
    <td class="tg-wruy">✔</td>
    <td class="tg-xwyw">32</td>
  </tr>
  <tr>
    <td class="tg-9wq8">Ascon [5]</td>
    <td class="tg-9wq8"><i>p<sup>12</sup></i></td>
    <td class="tg-wruy">✔</td>
    <td class="tg-xwyw">64</td>
  </tr>
  <tr>
    <td class="tg-9wq8">Elephant [13]</td>
    <td class="tg-9wq8">Spongent-π[160]</td>
    <td class="tg-73t7">✖</td>
    <td class="tg-xwyw">-</td>
  </tr>
  <tr>
    <td class="tg-9wq8">Gimli [8]</td>
    <td class="tg-9wq8">Gimli-36</td>
    <td class="tg-wruy">✔</td>
    <td class="tg-xwyw">32</td>
  </tr>
  <tr>
    <td class="tg-9wq8">Orange [20],<br>Photon-Beetle [9]</td>
    <td class="tg-9wq8">Photon-256</td>
    <td class="tg-73t7">✖</td>
    <td class="tg-xwyw">-</td>
  </tr>
  <tr>
    <td class="tg-9wq8">Xoodyak [15,16]</td>
    <td class="tg-9wq8">Xoodoo</td>
    <td class="tg-wruy">✔</td>
    <td class="tg-xwyw">32</td>
  </tr>
  <tr>
    <td class="tg-y0n7" colspan="4">others</td>
  </tr>
  <tr>
    <td class="tg-c3ow">Subterranean [14]</td>
    <td class="tg-9wq8">blank(8)</td>
    <td class="tg-73t7">✖</td>
    <td class="tg-xwyw">-</td>
  </tr>
</tbody>
</table>
</center>


Whenever possible, we generate both a bitsliced and an _n_-sliced
implementation for each primitive, which allows us to exercise the
bit-probing and the register-probing models of tightPROVE+. However, 4
primitives do not admit a straightforward n-sliced implementation. The
Subterranean permutation involves a significant amount of
bit-twiddling across its 257-bit state, which makes it a resolutely
bitsliced primitive (as confirmed by its reference
implementation). Photon, Skinny, Spongent rely on lookup tables that
would be too expansive to emulate in n-sliced mode. In bitslicing,
these tables are simply implemented by their Boolean circuit, either
provided by the authors (Photon, Skinny) or generated through SAT [2]
with the objective of minimizing multiplicative complexity (Spongent,
with 4 ANDs and 28 XORs).


Note that the _n_-sliced implementations, when they exist, are either
32-sliced or 64-sliced.  This means in particular that, unlike
bitslicing that processes multiple blocks in parallel, these
implementations process a single block at once on our 32-bit Cortex
M4. Note also that in practice, all _n_-sliced implementation are
vsliced, since the Cortex M4 we consider does not provide SIMD
extensions, which are required for hslicing.


Running tightPROVE<sup>+</sup> on our Usuba implementations showed
that all of them were _t_-probing secure in the bit-probing model, but
3 were not secure in the register-probing model. ACE required 384
additional refreshes, Clyde-128 required 6 refreshes, and Gimli
required 120. In the case of ACE and Clyde, the number of refreshes
inserted by tightPROVE<sup>+</sup> is known minimal, while for Gimli,
it is only an upper bound [3].


### Baseline Performance Evaluation

In the following, we benchmark our implementations –in Usuba and
compiled with Tornado– of the NIST submissions against the reference
implementation provided by the contestants. This allows us to
establish a performance baseline (without masking), thus providing a
common frame of reference for the performance of these primitives
based on their implementation synthesized from Usuba. In doing so, we
have to bear in mind that the reference implementations provided by
the NIST contestants are of varying quality: some appear to have been
finely tuned for performance while others focus on simplicity, acting
as an executable specification.

In an effort to level the playing field, we ran our benchmark on an
Intel i5-6500 @ 3.20GHz, running Linux 4.15.0-54. The implementations
were compiled with Clang 7.0.0 with flags `-O3 -fno-slp-vectorize
-fno-vectorize`. These flags prevent Clang from trying to produce
vectorized code, which would artificially advantage some
implementations at the expense of others because of brittle,
hard-to-predict vectorization heuristics. Besides, vectorized
instructions remain an exception in the setting of embedded devices
(_e.g._, Cortex M). At the exception of Subterranean (which is
bitsliced), the reference implementations follow a n-sliced
implementation pattern, representing the state of the primitive
through a matrix of 32-bit values, or 64-bit in the case of Ascon. To
evaluate bitsliced implementations, we simulate a 32-bit architecture,
meaning that the throughput we report corresponds to the parallel
encryption of 32 independent blocks. The cost of transposing data into
a bitslice format (around 9 cycles/bytes to transpose a 32x32 matrix)
is excluded. The results are shown in the following table:

<style type="text/css">
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  overflow:hidden;padding:10px 5px;word-break:normal;}
.tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}
.tg .tg-wa1i{font-weight:bold;text-align:center;vertical-align:middle}
.tg .tg-0lax{text-align:left;vertical-align:top}
</style>
<center>
<table class="tg" style="margin-bottom:25px;margin-top:25px">
<thead>
  <tr>
    <th class="tg-wa1i" rowspan="2">Primitive</th>
    <th class="tg-wa1i" colspan="3">Performances (cycles/bytes)<br><span style="font-style:italic">(lower is better)</span></th>
  </tr>
  <tr>
    <td class="tg-wa1i">Usuba n-slice</td>
    <td class="tg-wa1i">Usuba bitslice</td>
    <td class="tg-wa1i">reference</td>
  </tr>
</thead>
<tbody>
  <tr>
    <td class="tg-0lax">Ace</td>
    <td class="tg-0lax">34.25</td>
    <td class="tg-0lax">55.89</td>
    <td class="tg-0lax">273.53</td>
  </tr>
  <tr>
    <td class="tg-0lax">Ascon</td>
    <td class="tg-0lax">9.84</td>
    <td class="tg-0lax">4.94</td>
    <td class="tg-0lax">5.18</td>
  </tr>
  <tr>
    <td class="tg-0lax">Clyde</td>
    <td class="tg-0lax">33.72</td>
    <td class="tg-0lax">21.99</td>
    <td class="tg-0lax">37.69</td>
  </tr>
  <tr>
    <td class="tg-0lax">Gimli</td>
    <td class="tg-0lax">15.77</td>
    <td class="tg-0lax">5.80</td>
    <td class="tg-0lax">44.35</td>
  </tr>
  <tr>
    <td class="tg-0lax">Gift</td>
    <td class="tg-0lax">565.30</td>
    <td class="tg-0lax">45.51</td>
    <td class="tg-0lax">517.27</td>
  </tr>
  <tr>
    <td class="tg-0lax">Photon</td>
    <td class="tg-0lax">-</td>
    <td class="tg-0lax">44.88</td>
    <td class="tg-0lax">214.47</td>
  </tr>
  <tr>
    <td class="tg-0lax">Pyjamask</td>
    <td class="tg-0lax">246.72</td>
    <td class="tg-0lax">131.33</td>
    <td class="tg-0lax">267.35</td>
  </tr>
  <tr>
    <td class="tg-0lax">Skinny</td>
    <td class="tg-0lax">-</td>
    <td class="tg-0lax">46.87</td>
    <td class="tg-0lax">207.82</td>
  </tr>
  <tr>
    <td class="tg-0lax">Spongent</td>
    <td class="tg-0lax">-</td>
    <td class="tg-0lax">146.93</td>
    <td class="tg-0lax">4824.97</td>
  </tr>
  <tr>
    <td class="tg-0lax">Subterranean</td>
    <td class="tg-0lax">-</td>
    <td class="tg-0lax">17.64</td>
    <td class="tg-0lax">355.38</td>
  </tr>
  <tr>
    <td class="tg-0lax">Xoodoo</td>
    <td class="tg-0lax">14.93</td>
    <td class="tg-0lax">6.47</td>
    <td class="tg-0lax">10.14</td>
  </tr>
</tbody>
</table>
</center>

We notice that Usuba often delivers performance that is on par or
better than the reference implementations. Note that this does not
come at the expense of intelligibility: our Usuba implementations are
written in a high-level language, which is amenable to formal
reasoning thanks to its straightforward semantic model (unlike any
implementation in C). The reference implementations of Skinny and
Photon use lookup tables, which do not admit a straightforward
implementation in terms of constant-time, combinational operations. As
a result, we are unable to implement a constant-time n-sliced version
in Usuba and to mask such an implementation.

We now turn our attention specifically to a few implementations that
exhibit interesting performance with the following observations:

 - The reference implementation of Subterranean is an order of
   magnitude slower than in Usuba because its implementation is
   bit-oriented (each bit is stored in a distinct 8-bit variable) but
   only a single block is encrypted at a time. Switching to 32-bit
   variables and encrypting 32 blocks in parallel, as Usuba does,
   significantly improves performance.
   
 - The reference implementation of Spongent is slowed down by a
   prohibitively expensive bitpermutation over 160 bits, which is
   spread across 20 8-bit variables. Thanks to bitslicing, Usuba turns
   this permutation into a purely static renaming of variable, which
   occurs purely at compile-time.
   
 - On Ascon, our n-sliced implementation is twice slower than the
   reference implementation. Unlike the reference implementation, we
   have refrained from performing aggressive function inlining and
   loop unrolling to keep code size in check, since we target embedded
   systems.  However, if we instruct the Usuba compiler to perform
   these optimizations, the performance of our n-sliced implementation
   is on par with the reference one.
   
 - Ace reference implementation suffers from significant performance
   issues, relying on an excessive number of temporary variables to
   store intermediate results.
   
 - Gimli offers two reference implementations, one being a
   high-performance SSE implementation with the other serving as an
   executable specification on general-purpose registers.  We chose
   the general-purpose one here (which had not been subjected to the
   same level of optimizations) because our target architecture
   (Cortex M) does not provide a vectorized instruction set.
   
 - Finally, Gift's n-slice implementation suffers from sever
   performance issues because of its expensive linear layer. Using a
   the recent _fixslicing_ technique [21] improves the performance of
   Usuba's _n_-slice Gift implementation down to 42 cycles/byte.


### Masking benchmarks

We ran our benchmarks on a Nucleo STM32F401RE offering an Arm
Cortex-M4 with 512 Kbytes of Flash memory and 96 Kbytes of SRAM. We
used the GNU C compiler arm-none-eabi-gcc version 9.2.0 at
optimization level -O3.  We considered two modes regarding the Random
Number Generator (RNG):


 - Pooling mode: The RNG generates random numbers at a rate of 32 bits
   every 64 clock cycles.  Fetching a random number can thus take up
   to 65 clock cycles.

 - Fast mode: The RNG only takes a few clock cycles to generate a
   32-bit random word. The RNG routine thus can simply read a register
   containing this 32-bit random word without checking for its
   availability.


Those two modes were chosen because they are the ones used in the
submission of Pyjamask, which is the only submission detailing the
question of how to get random numbers for a masked implementation.


#### Scaling




<!--
Since masking a multiplication has a quadratic cost in the number of
shares, we expect performance at high orders to be mostly proportional
with the number of multiplications used by the primitives. We thus
report the number of multiplications involved in our implementation
(bitsliced and nsliced) in the following table:


<style type="text/css">
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  overflow:hidden;padding:10px 5px;word-break:normal;}
.tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}
.tg .tg-wa1i{font-weight:bold;text-align:center;vertical-align:middle}
.tg .tg-uzvj{border-color:inherit;font-weight:bold;text-align:center;vertical-align:middle}
.tg .tg-0pky{border-color:inherit;text-align:left;vertical-align:top}
.tg .tg-0lax{text-align:left;vertical-align:top}
</style>
<center>
<table class="tg">
<thead>
  <tr>
    <th class="tg-uzvj" rowspan="2">primitive</th>
    <th class="tg-wa1i" rowspan="2">state size<br>(bits)</th>
    <th class="tg-uzvj" colspan="2">multiplications</th>
    <th class="tg-uzvj" colspan="2">multiplications/bits</th>
  </tr>
  <tr>
    <td class="tg-wa1i">n-slice</td>
    <td class="tg-wa1i">bitslice</td>
    <td class="tg-wa1i">n-slice</td>
    <td class="tg-wa1i">bitslice</td>
  </tr>
</thead>
<tbody>
  <tr>
    <td class="tg-0pky">ACE</td>
    <td class="tg-0lax">320<br></td>
    <td class="tg-0pky">384</td>
    <td class="tg-0lax">12288</td>
    <td class="tg-0pky">1.2</td>
    <td class="tg-0lax">38</td>
  </tr>
  <tr>
    <td class="tg-0pky">ASCON</td>
    <td class="tg-0lax">320</td>
    <td class="tg-0pky">60</td>
    <td class="tg-0lax">3840</td>
    <td class="tg-0pky">0.19</td>
    <td class="tg-0lax">12</td>
  </tr>
  <tr>
    <td class="tg-0pky">Clyde</td>
    <td class="tg-0lax">128</td>
    <td class="tg-0pky">48</td>
    <td class="tg-0lax">1536</td>
    <td class="tg-0pky">0.37</td>
    <td class="tg-0lax">12</td>
  </tr>
  <tr>
    <td class="tg-0pky">GIFT</td>
    <td class="tg-0lax">128</td>
    <td class="tg-0pky">160</td>
    <td class="tg-0lax">5120</td>
    <td class="tg-0pky">1.25</td>
    <td class="tg-0lax">40</td>
  </tr>
  <tr>
    <td class="tg-0pky">Gimli</td>
    <td class="tg-0lax">384</td>
    <td class="tg-0pky">288</td>
    <td class="tg-0lax">9216</td>
    <td class="tg-0pky">0.75</td>
    <td class="tg-0lax">24</td>
  </tr>
  <tr>
    <td class="tg-0pky">PHOTON</td>
    <td class="tg-0lax">256</td>
    <td class="tg-0pky">-</td>
    <td class="tg-0lax">3072</td>
    <td class="tg-0pky">-</td>
    <td class="tg-0lax">12</td>
  </tr>
  <tr>
    <td class="tg-0pky">Pyjamask</td>
    <td class="tg-0lax">128</td>
    <td class="tg-0pky">56</td>
    <td class="tg-0lax">1792</td>
    <td class="tg-0pky">0.44</td>
    <td class="tg-0lax">14</td>
  </tr>
  <tr>
    <td class="tg-0pky">SKINNY</td>
    <td class="tg-0lax">128</td>
    <td class="tg-0pky">-</td>
    <td class="tg-0lax">6144</td>
    <td class="tg-0pky">-</td>
    <td class="tg-0lax">48</td>
  </tr>
  <tr>
    <td class="tg-0pky">SPONGENT</td>
    <td class="tg-0lax">160</td>
    <td class="tg-0pky">-</td>
    <td class="tg-0lax">12800</td>
    <td class="tg-0pky">-</td>
    <td class="tg-0lax">80</td>
  </tr>
  <tr>
    <td class="tg-0pky">Subterranean</td>
    <td class="tg-0lax">257</td>
    <td class="tg-0pky">-</td>
    <td class="tg-0lax">2056</td>
    <td class="tg-0pky">-</td>
    <td class="tg-0lax">8</td>
  </tr>
  <tr>
    <td class="tg-0pky">Xoodoo</td>
    <td class="tg-0lax">384</td>
    <td class="tg-0pky">144</td>
    <td class="tg-0lax">4608</td>
    <td class="tg-0pky">0.37</td>
    <td class="tg-0lax">12</td>
  </tr>
</tbody>
</table>
</center>
-->


#### N-slicing

The table bellow gives the performances of the _n_-sliced
implementations produced by Tornado in terms of cycles per bloc. Note
that this does not reflect the performances in term of cycles/bytes,
since those ciphers have different bloc size. For instance, encrypting
a single bloc with Xoodoo is more than twice slower than with Clyde
(at any given order), but since Xoodoo's bloc is 384 bits, while
Clyde's is only 128 bits, the performances of Xoodoo in terms of
cycles/bytes are _better_ than Clyde's. Nevertheless, we focus here on
the performances per bloc, and analyze the performances per bytes in
the next section, when considering bitsliced implementations.


<style type="text/css">
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  overflow:hidden;padding:10px 5px;word-break:normal;}
.tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}
.tg .tg-1wig{font-weight:bold;text-align:left;vertical-align:top}
.tg .tg-wa1i{font-weight:bold;text-align:center;vertical-align:middle}
.tg .tg-nrix{font-weight:bold;text-align:center;vertical-align:middle}
.tg .tg-0lax{text-align:left;vertical-align:top}
.tg .tg-0qe0{background-color:#ecf4ff;text-align:left;vertical-align:top}
.tg .tg-2col{background-color:#efefef;text-align:center;vertical-align:middle}
</style>
<center>
<table class="tg" style="margin-bottom:25px;margin-top:25px">
<thead>
  <tr>
    <th class="tg-nrix" rowspan="2">primitive</th>
    <th class="tg-nrix" rowspan="2">mults</th>
    <th class="tg-nrix" rowspan="2">RNG<br>mode</th>
    <th class="tg-nrix" colspan="7">Performances (cycles)<br><span style="font-style:italic">(lower is better)</span></th>
  </tr>
  <tr>
    <th class="tg-nrix"><span style="font-style:italic">d</span> = 0</th>
    <th class="tg-nrix"><span style="font-style:italic">d</span> = 3</th>
    <th class="tg-nrix"><span style="font-style:italic">d</span> = 7</th>
    <th class="tg-nrix"><span style="font-style:italic">d</span> = 15</th>
    <th class="tg-nrix"><span style="font-style:italic">d</span> = 31</th>
    <th class="tg-nrix"><span style="font-style:italic">d</span> = 63</th>
    <th class="tg-nrix"><span style="font-style:italic">d</span> = 127</th>
  </tr>
</thead>
<tbody>
  <tr>
    <td class="tg-2col" rowspan="2">Clyde</td>
    <td class="tg-2col" rowspan="2">48</td>
    <td class="tg-0qe0">fast</td>
    <td class="tg-0qe0">1.47k</td>
    <td class="tg-0qe0">15.38k</td>
    <td class="tg-0qe0">56.48k</td>
    <td class="tg-0qe0">189.44k</td>
    <td class="tg-0qe0">670.08k</td>
    <td class="tg-0qe0">2.58m</td>
    <td class="tg-0qe0">10.45m</td>
  </tr>
  <tr>
    <td class="tg-0lax">pooling</td>
    <td class="tg-0lax">1.47k</td>
    <td class="tg-0lax">30.08k</td>
    <td class="tg-0lax">121.08k</td>
    <td class="tg-0lax">502.88k</td>
    <td class="tg-0lax">1.94m</td>
    <td class="tg-0lax">7.73m</td>
    <td class="tg-0lax">29.92m</td>
  </tr>
  
  <tr>
    <td class="tg-2col" rowspan="2">Pyjamask</td>
    <td class="tg-2col" rowspan="2">56</td>
    <td class="tg-0qe0">fast</td>
    <td class="tg-0qe0">15.90k</td>
    <td class="tg-0qe0">79.52k</td>
    <td class="tg-0qe0">205.44k</td>
    <td class="tg-0qe0">614.40k</td>
    <td class="tg-0qe0">1.73m</td>
    <td class="tg-0qe0">4.75m</td>
    <td class="tg-0qe0">15.20m</td>
  </tr>
  <tr>
    <td class="tg-0lax">pooling</td>
    <td class="tg-0lax">15.90k</td>
    <td class="tg-0lax">94.88k</td>
    <td class="tg-0lax">274.56k</td>
    <td class="tg-0lax">954.56k</td>
    <td class="tg-0lax">3.10m</td>
    <td class="tg-0lax">10.34m</td>
    <td class="tg-0lax">36.32m</td>
  </tr>

  <tr>
    <td class="tg-2col" rowspan="2">Ascon</td>
    <td class="tg-2col" rowspan="2">60</td>
    <td class="tg-0qe0">fast</td>
    <td class="tg-0qe0">1.96k</td>
    <td class="tg-0qe0">42.00k</td>
    <td class="tg-0qe0">123.20k</td>
    <td class="tg-0qe0">464.40k</td>
    <td class="tg-0qe0">1.70m</td>
    <td class="tg-0qe0">6.52m</td>
    <td class="tg-0qe0">25.60m</td>
  </tr>
  <tr>
    <td class="tg-0lax">pooling</td>
    <td class="tg-0lax">1.96k</td>
    <td class="tg-0lax">53.60k</td>
    <td class="tg-0lax">182.80k</td>
    <td class="tg-0lax">821.60k</td>
    <td class="tg-0lax">3.17m</td>
    <td class="tg-0lax">12.96m</td>
    <td class="tg-0lax">52.00m</td>
  </tr>

  <tr>
    <td class="tg-2col" rowspan="2">Xoodoo</td>
    <td class="tg-2col" rowspan="2">144</td>
    <td class="tg-0qe0">fast</td>
    <td class="tg-0qe0">3.02k</td>
    <td class="tg-0qe0">42.67k</td>
    <td class="tg-0qe0">156.48k</td>
    <td class="tg-0qe0">520.32k</td>
    <td class="tg-0qe0">1.89m</td>
    <td class="tg-0qe0">6.86m</td>
    <td class="tg-0qe0">26.64m</td>
  </tr>
  <tr>
    <td class="tg-0lax">pooling</td>
    <td class="tg-0lax">3.02k</td>
    <td class="tg-0lax">82.08k</td>
    <td class="tg-0lax">334.08k</td>
    <td class="tg-0lax">1.40m</td>
    <td class="tg-0lax">5.42m</td>
    <td class="tg-0lax">21.50m</td>
    <td class="tg-0lax">83.04m</td>
  </tr>
  
  <tr>
    <td class="tg-2col" rowspan="2">Gift</td>
    <td class="tg-2col" rowspan="2">160</td>
    <td class="tg-0qe0">fast</td>
    <td class="tg-0qe0">17.92k</td>
    <td class="tg-0qe0">200.48k</td>
    <td class="tg-0qe0">516.32k</td>
    <td class="tg-0qe0">1.24m</td>
    <td class="tg-0qe0">6.45m</td>
    <td class="tg-0qe0">13.10m</td>
    <td class="tg-0qe0">42.24m</td>
  </tr>
  <tr>
    <td class="tg-0lax">pooling</td>
    <td class="tg-0lax">17.92k</td>
    <td class="tg-0lax">244.32k</td>
    <td class="tg-0lax">714.88k</td>
    <td class="tg-0lax">2.21m</td>
    <td class="tg-0lax">8.51m</td>
    <td class="tg-0lax">29.12m</td>
    <td class="tg-0lax">102.40m</td>
  </tr>
  
  <tr>
    <td class="tg-2col" rowspan="2">Gimli</td>
    <td class="tg-2col" rowspan="2">288</td>
    <td class="tg-0qe0">fast</td>
    <td class="tg-0qe0">2.69k</td>
    <td class="tg-0qe0">84.96k</td>
    <td class="tg-0qe0">342.72k</td>
    <td class="tg-0qe0">1.19m</td>
    <td class="tg-0qe0">4.57m</td>
    <td class="tg-0qe0">17.09m</td>
    <td class="tg-0qe0">67.20m</td>
  </tr>
  <tr>
    <td class="tg-0lax">pooling</td>
    <td class="tg-0lax">2.69k</td>
    <td class="tg-0lax">190.56k</td>
    <td class="tg-0lax">832.80k</td>
    <td class="tg-0lax">3.52m</td>
    <td class="tg-0lax">14.06m</td>
    <td class="tg-0lax">56.16m</td>
    <td class="tg-0lax">218.88m</td>
  </tr>
  
  <tr>
    <td class="tg-2col" rowspan="2">Ace</td>
    <td class="tg-2col" rowspan="2">384</td>
    <td class="tg-0qe0">fast</td>
    <td class="tg-0qe0">3.68k</td>
    <td class="tg-0qe0">155.20k</td>
    <td class="tg-0qe0">531.60k</td>
    <td class="tg-0qe0">1.60m</td>
    <td class="tg-0qe0">7.60m</td>
    <td class="tg-0qe0">29.84m</td>
    <td class="tg-0qe0">113.60m</td>
  </tr>
  <tr>
    <td class="tg-0lax">pooling</td>
    <td class="tg-0lax">3.68k</td>
    <td class="tg-0lax">302.00k</td>
    <td class="tg-0lax">1.32m</td>
    <td class="tg-0lax">4.56m</td>
    <td class="tg-0lax">19.80m</td>
    <td class="tg-0lax">78.40m</td>
    <td class="tg-0lax">310.80m</td>
  </tr>
</tbody>
</table>
</center>


Since masking a multiplication has a quadratic cost in the number of
shares, we expect performance at high orders to be mostly proportional
with the number of multiplications used by the primitives. We thus
report the number of multiplications involved in our implementations
of the primitives. This is confirmed by our results with 128 shares
(on the Cortex M4). This effect is less pronounced at small orders
since the execution time remains dominated by linear operations. Using
the pooling RNG increases the cost of multiplications compared to the
fast RNG, which results in performances being proportional to the
number of multiplications at smaller order than with the fast RNG.

Pyjamask illustrates the influence of the number of multiplications on
scaling. Because of its use of dense binary matrix multiplications, it
involves a significant number of linear operations for only a few
multiplications. As a result, it is slower than Xoodoo order 3,
despite the fact that Xoodoo uses 2.5× more multiplications. With the
fast RNG, the inflection point is reached at order 31 only to improve
afterward. Similarly when compared to Clyde, Pyjamask goes from 5×
slower at order 3 to 50% slower at order 127 with the fast RNG and 20%
slower at order 127 with the pooling RNG. The same analysis applies to
Gift and Gimli, where the linear overhead of Gift is only dominated at
order 7 with the pooling RNG and at order 63 with the fast RNG.

One notable exception is Ascon with the fast RNG, compared in
particular to Clyde and Pyjamask. Despite Ascon using only slightly
more multiplications, it involves a 64-sliced implementation, unlike
its counterparts that are 32-sliced. Running on our 32-bit Cortex-M4
requires GCC to generate 64-bit emulation code, which induces a
significant operational overhead and prevents further optimization by
the compiler. Regardless of the RNG used, Ascon is thus significantly
slower than Clyde (at order 3 at beyond) and Pyjamask (at order 63 and
beyond).


<!-- <p align="center" style="margin-top:30px;margin-bottom:30px;"> -->
<!-- <img style="height:auto;width:auto;max-width:100%" src="{{ site.baseurl }}/assets/images/blog/n-slice-masking-scaling-bloc-pooling.png"> -->
<!-- </p> -->

<!-- <p align="center" style="margin-top:30px;margin-bottom:30px"> -->
<!-- <img style="height:auto;width:auto;max-width:100%" src="{{ site.baseurl }}/assets/images/blog/n-slice-masking-scaling-bloc-fast.png"> -->
<!-- </p> -->

<!-- <p align="center" style="margin-top:30px;margin-bottom:30px;"> -->
<!-- <img style="height:auto;width:auto;max-width:100%" src="{{ site.baseurl }}/assets/images/blog/n-slice-masking-scaling-byte-pooling.png"> -->
<!-- </p> -->

<!-- <p align="center" style="margin-top:30px;margin-bottom:30px"> -->
<!-- <img style="height:auto;width:auto;max-width:100%" src="{{ site.baseurl }}/assets/images/blog/n-slice-masking-scaling-byte-fast.png"> -->
<!-- </p> -->


#### Bitslicing

The key limiting factor to execute bitslice code on an embedded device
is the amount of memory available. Bitsliced programs tend to be large
and to consume a significant amount of stack. Masking such
implementations at high orders becomes quickly impractical because of
the quadratic growth of the stack usage.

To reduce stack usage and allow us to explore high masking orders, our
bitsliced programs manipulate 8-bit variables, meaning that 8
independent blocks can be processed in parallel. This trades memory
usage for performance, as we could have used 32-bit variables and
improved our throughput by a factor 4. However, doing so would have
put an unbearable amount of pressure on the stack, which would have
prevented us from considering masking orders beyond 7. Besides, it is
not clear whether there is a use-case for such a massively parallel
(32 independent blocks) encryption primitive in a lightweight
setting. As a result of our compilation strategy, we have been able to
mask all primitives with up to 16 shares and, additionally, reach 32
shares for Photon, Skinny, Spongent and Subterranean.

<style type="text/css">
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  overflow:hidden;padding:10px 5px;word-break:normal;}
.tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}
.tg .tg-wa1i{font-weight:bold;text-align:center;vertical-align:middle}
.tg .tg-0lax{text-align:left;vertical-align:top}
</style>
<center>
<table class="tg" style="margin-bottom:25px;margin-top:25px">
<thead>
  <tr>
    <th class="tg-wa1i" colspan="7">Bitslice performances (fast RNG)</th>
  </tr>
  <tr>
    <th class="tg-wa1i" rowspan="2">primitive</th>
    <th class="tg-wa1i" rowspan="2">mults/bits</th>
    <th class="tg-wa1i" colspan="5">Performances (cycles/bytes)<br><span style="font-style:italic">(lower is better)</span></th>
  </tr>
  <tr>
    <td class="tg-wa1i"><span style="font-style:italic">d</span> = 0</td>
    <td class="tg-wa1i"><span style="font-style:italic">d</span> = 3</td>
    <td class="tg-wa1i"><span style="font-style:italic">d</span> = 7</td>
    <td class="tg-wa1i"><span style="font-style:italic">d</span> = 16</td>
    <td class="tg-wa1i"><span style="font-style:italic">d </span>= 32</td>
  </tr>
</thead>
<tbody>
  <tr>
    <td class="tg-0lax">Subterranean</td>
    <td class="tg-0lax">8</td>
    <td class="tg-0lax">94</td>
    <td class="tg-0lax">2.15k</td>
    <td class="tg-0lax">7.18k</td>
    <td class="tg-0lax">27.23k</td>
    <td class="tg-0lax">95.19k</td>
  </tr>
  <tr>
    <td class="tg-0lax">Ascon</td>
    <td class="tg-0lax">12</td>
    <td class="tg-0lax">101</td>
    <td class="tg-0lax">3.07k</td>
    <td class="tg-0lax">11.45k</td>
    <td class="tg-0lax">42.39k</td>
    <td class="tg-0lax"></td>
  </tr>
  <tr>
    <td class="tg-0lax">Xoodoo</td>
    <td class="tg-0lax">12</td>
    <td class="tg-0lax">112</td>
    <td class="tg-0lax">3.12k</td>
    <td class="tg-0lax">10.49k</td>
    <td class="tg-0lax">38.35k</td>
    <td class="tg-0lax"></td>
  </tr>
  <tr>
    <td class="tg-0lax">Clyde</td>
    <td class="tg-0lax">12</td>
    <td class="tg-0lax">177</td>
    <td class="tg-0lax">3.44k</td>
    <td class="tg-0lax">13.57k</td>
    <td class="tg-0lax">45.34k</td>
    <td class="tg-0lax"></td>
  </tr>
  <tr>
    <td class="tg-0lax">Photon</td>
    <td class="tg-0lax">12</td>
    <td class="tg-0lax">193</td>
    <td class="tg-0lax">7.66k</td>
    <td class="tg-0lax">14.28k</td>
    <td class="tg-0lax">44.99k</td>
    <td class="tg-0lax">154k</td>
  </tr>
  <tr>
    <td class="tg-0lax">Pyjamask</td>
    <td class="tg-0lax">14</td>
    <td class="tg-0lax">1.59k</td>
    <td class="tg-0lax">16.52k</td>
    <td class="tg-0lax">31.76k</td>
    <td class="tg-0lax">97.88k</td>
    <td class="tg-0lax"></td>
  </tr>
  <tr>
    <td class="tg-0lax">Gimli</td>
    <td class="tg-0lax">24</td>
    <td class="tg-0lax">127</td>
    <td class="tg-0lax">5.51k</td>
    <td class="tg-0lax">19.15k</td>
    <td class="tg-0lax">76.91k</td>
    <td class="tg-0lax"></td>
  </tr>
  <tr>
    <td class="tg-0lax">Ace</td>
    <td class="tg-0lax">38</td>
    <td class="tg-0lax">336</td>
    <td class="tg-0lax">8.22k</td>
    <td class="tg-0lax">35.29k</td>
    <td class="tg-0lax">123k</td>
    <td class="tg-0lax"></td>
  </tr>
  <tr>
    <td class="tg-0lax">Gift</td>
    <td class="tg-0lax">40</td>
    <td class="tg-0lax">358</td>
    <td class="tg-0lax">11.08k</td>
    <td class="tg-0lax">36.79k</td>
    <td class="tg-0lax">136k</td>
    <td class="tg-0lax"></td>
  </tr>
  <tr>
    <td class="tg-0lax">Skinny</td>
    <td class="tg-0lax">48</td>
    <td class="tg-0lax">441</td>
    <td class="tg-0lax">18.19k</td>
    <td class="tg-0lax">61.75k</td>
    <td class="tg-0lax">200k</td>
    <td class="tg-0lax">664k</td>
  </tr>
  <tr>
    <td class="tg-0lax">Spongent</td>
    <td class="tg-0lax">80</td>
    <td class="tg-0lax">624</td>
    <td class="tg-0lax">19.45k</td>
    <td class="tg-0lax">64.78k</td>
    <td class="tg-0lax">259k</td>
    <td class="tg-0lax">948k</td>
  </tr>
</tbody>
</table>
</center>

As for the n-sliced implementations, we observe a close match between
the asymptotic performance of the primitive and their number of
multiplications per bits, which becomes even more prevalent as order
increases and the overhead of linear operations becomes comparatively
smaller. Pyjamask remains a good example to illustrate this
phenomenon, the inflection point being reached at order 15 with
respect to Ace (which uses 3× more multiplications).

The performance of Ascon with the fast RNG, which was slowed down by
its suboptimal use of 64-bit registers in n-slicing, is streamlined in
bitslicing: here, it exhibits the same number of multiplication per
bits as Xoodoo and, indeed, their performance match remarkably well.

<style type="text/css">
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  overflow:hidden;padding:10px 5px;word-break:normal;}
.tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}
.tg .tg-wa1i{font-weight:bold;text-align:center;vertical-align:middle}
.tg .tg-0lax{text-align:left;vertical-align:top}
</style>
<center>
<table class="tg" style="margin-bottom:25px;margin-top:25px">
<thead>
  <tr>
    <th class="tg-wa1i" colspan="7">Bitslice performances (pooling RNG)</th>
  </tr>
  <tr>
    <th class="tg-wa1i" rowspan="2">primitive</th>
    <th class="tg-wa1i" rowspan="2">mults/bits</th>
    <th class="tg-wa1i" colspan="5">Performances (cycles/bytes)<br><span style="font-style:italic">(lower is better)</span></th>
  </tr>
  <tr>
    <td class="tg-wa1i"><span style="font-style:italic">d</span> = 0</td>
    <td class="tg-wa1i"><span style="font-style:italic">d</span> = 3</td>
    <td class="tg-wa1i"><span style="font-style:italic">d</span> = 7</td>
    <td class="tg-wa1i"><span style="font-style:italic">d</span> = 16</td>
    <td class="tg-wa1i"><span style="font-style:italic">d </span>= 32</td>
  </tr>
</thead>
<tbody>
  <tr>
    <td class="tg-0lax">Subterranean</td>
    <td class="tg-0lax">8</td>
    <td class="tg-0lax">94</td>
    <td class="tg-0lax">4.46k</td>
    <td class="tg-0lax">19.13k</td>
    <td class="tg-0lax">79.63k</td>
    <td class="tg-0lax">312k</td>
  </tr>
  <tr>
    <td class="tg-0lax">Ascon</td>
    <td class="tg-0lax">12</td>
    <td class="tg-0lax">101</td>
    <td class="tg-0lax">7.33k</td>
    <td class="tg-0lax">30.33k</td>
    <td class="tg-0lax">125k</td>
    <td class="tg-0lax"></td>
  </tr>
  <tr>
    <td class="tg-0lax">Xoodoo</td>
    <td class="tg-0lax">12</td>
    <td class="tg-0lax">112</td>
    <td class="tg-0lax">6.69k</td>
    <td class="tg-0lax">28.79K</td>
    <td class="tg-0lax">120K</td>
    <td class="tg-0lax"></td>
  </tr>
  <tr>
    <td class="tg-0lax">Clyde</td>
    <td class="tg-0lax">12</td>
    <td class="tg-0lax">177</td>
    <td class="tg-0lax">7.88k</td>
    <td class="tg-0lax">31.04k</td>
    <td class="tg-0lax">127k</td>
    <td class="tg-0lax"></td>
  </tr>
  <tr>
    <td class="tg-0lax">Photon</td>
    <td class="tg-0lax">12</td>
    <td class="tg-0lax">193</td>
    <td class="tg-0lax">10.47k</td>
    <td class="tg-0lax">31.77k</td>
    <td class="tg-0lax">126k</td>
    <td class="tg-0lax">476k</td>
  </tr>
  <tr>
    <td class="tg-0lax">Pyjamask</td>
    <td class="tg-0lax">14</td>
    <td class="tg-0lax">1.59k</td>
    <td class="tg-0lax">20.33k</td>
    <td class="tg-0lax">52.81k</td>
    <td class="tg-0lax">193k</td>
    <td class="tg-0lax"></td>
  </tr>
  <tr>
    <td class="tg-0lax">Gimli</td>
    <td class="tg-0lax">24</td>
    <td class="tg-0lax">127</td>
    <td class="tg-0lax">12.14k</td>
    <td class="tg-0lax">53.64k</td>
    <td class="tg-0lax">236k</td>
    <td class="tg-0lax"></td>
  </tr>
  <tr>
    <td class="tg-0lax">Ace</td>
    <td class="tg-0lax">38</td>
    <td class="tg-0lax">336</td>
    <td class="tg-0lax">19.94k</td>
    <td class="tg-0lax">89.12k</td>
    <td class="tg-0lax">395k</td>
    <td class="tg-0lax"></td>
  </tr>
  <tr>
    <td class="tg-0lax">Gift</td>
    <td class="tg-0lax">40</td>
    <td class="tg-0lax">358</td>
    <td class="tg-0lax">21.38k</td>
    <td class="tg-0lax">93.92k</td>
    <td class="tg-0lax">405k</td>
    <td class="tg-0lax"></td>
  </tr>
  <tr>
    <td class="tg-0lax">Skinny</td>
    <td class="tg-0lax">48</td>
    <td class="tg-0lax">441</td>
    <td class="tg-0lax">34.28k</td>
    <td class="tg-0lax">131k</td>
    <td class="tg-0lax">525k</td>
    <td class="tg-0lax">1.97m</td>
  </tr>
  <tr>
    <td class="tg-0lax">Spongent</td>
    <td class="tg-0lax">80</td>
    <td class="tg-0lax">624</td>
    <td class="tg-0lax">44.04k</td>
    <td class="tg-0lax">188k</td>
    <td class="tg-0lax">816k</td>
    <td class="tg-0lax">3.15m</td>
  </tr>
</tbody>
</table>
</center>

Finally, we observe that with the pooling RNG, already at order 15,
the performances of our implementations is in accord with their
relative number of multiplications per bits. In bitslicing (more
evidently than in n-slicing), the number of multiplications is
performance critical, even at relatively low masking order.



### Usuba vs reference and future work


Of these 11 NIST submissions, only Pyjamask provides a masked
implementation. Our implementation is consistently (at every order,
and with both the pooling and fast RNGs) 1.8 times slower than their
masked implementation. The main reason is that they manually
implemented in assembly the two most used routines.

The first of those two routines is the matrix multiplication. The
Usuba code for this matrix multiplication is:

```lustre

node mat_mult(col:u32,vec:u32) returns (res:u32)
vars
    mask:u32
let
    res  = 0;

    forall i in [0, 31] {
        mask    := bitmask(vec,i);
        res     := res ^ (mask & mat_col);
        mat_col := mat_col[i] >>> 1;
    }
tel
```

When compiled with gcc arm-none-eabi-gcc 4.9.3, the body of the loop
is compiled to 6 instructions. The reference implementation on the
other hand inlines the loop and using ARM's barrel shifter, they are
able to compute each iteration in only 3 instructions. Using the
hand-tuned matrix multiplication Usuba's matrix multiplication offers
a speedup of 15% to 52%, depending on the masking order, as shown in
the Table below:

<style type="text/css">
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  overflow:hidden;padding:10px 5px;word-break:normal;}
.tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}
.tg .tg-1wig{font-weight:bold;text-align:left;vertical-align:top}
.tg .tg-baqh{text-align:center;vertical-align:top}
</style>
<center>
<table class="tg" style="margin-bottom:25px;margin-top:25px">
<thead>
  <tr>
    <th class="tg-1wig">masking order</th>
    <th class="tg-baqh">3</th>
    <th class="tg-baqh">7</th>
    <th class="tg-baqh">15</th>
    <th class="tg-baqh">31</th>
    <th class="tg-baqh">63</th>
    <th class="tg-baqh">127</th>
  </tr>
</thead>
<tbody>
  <tr>
    <td class="tg-1wig">speedup</td>
    <td class="tg-baqh">52%</td>
    <td class="tg-baqh">49%</td>
    <td class="tg-baqh">43%</td>
    <td class="tg-baqh">34%</td>
    <td class="tg-baqh">24%</td>
    <td class="tg-baqh">15%</td>
  </tr>
</tbody>
</table>
</center>

The slowdown is less important at high masking orders because this
matrix multiplication has a linear cost in the masking order, while
the masked multiplication has a quadratic cost in the masking order
(since it must compute all cross products between two shared values).

The second hand-optimized routine is the masked multiplication. The
authors of the reference implementation used the same ISW
multiplication gadget as we used in Usuba (shown in Section
[Masking](#masking)), but manually optimized it in assembly. They
unrolled twice the main loop of the multiplication, then fused the two
resulting inner loops, and carefully assigned 6 variables to
registers. If we note _m_ the masking order, this optimization allows
the masked multiplication to require _m_ less jumps (_m/2_ for inner
loops and _m/2_ outer loops), to use _m/2 * 10_ less memory accesses
thanks to their manual assignment of array values to registers, and to
require _m/2*3_ less memory accesses thanks to the merged inner
loops. gcc is unable to automatically perform such optimization.

Unlike the hand-optimized matrix multiplication, whose benefits
diminish as the masking order increases, the optimized masked
multiplication is more beneficial at higher masking order, as can be
seen from the Table below:
 
<style type="text/css">
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  overflow:hidden;padding:10px 5px;word-break:normal;}
.tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}
.tg .tg-1wig{font-weight:bold;text-align:left;vertical-align:top}
.tg .tg-baqh{text-align:center;vertical-align:top}
</style>
<center>
<table class="tg" style="margin-bottom:25px;margin-top:25px">
<thead>
  <tr>
    <th class="tg-1wig">masking order</th>
    <th class="tg-baqh">3</th>
    <th class="tg-baqh">7</th>
    <th class="tg-baqh">15</th>
    <th class="tg-baqh">31</th>
    <th class="tg-baqh">63</th>
    <th class="tg-baqh">127</th>
  </tr>
</thead>
<tbody>
  <tr>
    <td class="tg-1wig">speedup</td>
    <td class="tg-baqh">14%</td>
    <td class="tg-baqh">21%</td>
    <td class="tg-baqh">33%</td>
    <td class="tg-baqh">37%</td>
    <td class="tg-baqh">39%</td>
    <td class="tg-baqh">41%</td>
  </tr>
</tbody>
</table>
</center>


While the Usuba implementations can be sped up by using the hand-tuned
version of the ISW masked multiplication, those two examples (matrix
multiplication and masked multiplication) raise the concern that
high-performance cryptography might not be achieved on embedded
devices using C. Indeed, while the superscalar nature of high-end CPUs
can make up for occational non-optimal codes generated by C compilers,
embedded devices are less forgiving. Similarly, Schwabe and Stoffelen
[22] designed high-performance implementations of AES on Cortex-M3 and
M4 microprocessors, and prefered to write their own register allocator
and instruction scheduler (in less that 250 lines of Python),
suggesting that gcc's register allocator and instruction scheduler may
be far from optimal for cryptography on embedded devices.

We leave for future work to directly target ARM assembly in Usuba, and
implement architecture-specific optimizations for embedded
microprocessors. The end of the pipeline of Usubac incorporating those
modifications should be similar as shown in the figure below:


<p align="center" style="margin-top:30px;margin-bottom:30px;">
<img style="height:auto;width:auto;max-width:100%" src="{{ site.baseurl }}/assets/images/blog/usuba-to-asm-pipeline.png">
</p>

As of now, code-generation is the very last pass of Usubac, and does
not go through its own intermediate representation (IR): the C code is
generated directly from the Usuba AST. Optimizing down to assembly
would require a "lowering" pass that would convert the Usuba AST into
a pseudo-assembly AST (which would represent AST but would use virtual
registers rather than machine registers since register-allocation has
not happened yet). Then, several optimization passes would be
needed. Among the most important, a _peephole optimizer_ to recognize
and optimize commonly used idioms. For instance, a shift followed by
an instruction using that shift could be optimized into a single
instruction using the barrel shifter. Finally, register allocation and
instruction scheduling (performed either separately or together) would
produce an AST representing real machine code, which could then be
printed to produce an assembly file. Each of those passes would be
architecture-specific.


---
## References

[1] Y. Ishai _et al._, [Private Circuits: Securing Hardware against Probing Attacks](https://people.eecs.berkeley.edu/~daw/papers/privcirc-crypto03.pdf), CRYPTO, 2003.

[2] K. Stoffelen, [Efficient cryptography on the RISC-V architecture](https://eprint.iacr.org/2019/794.pdf), LATINCRYPT, 2019.

[3] S. Belaïd _et al._, [Tornado: Automatic Generation of Probing-Secure Masked Bitsliced Implementations](https://eprint.iacr.org/2020/506.pdf), EUROCRYPT, 2020.

[4] M. Aagaard _et al._, [ACE: An Authenticated Encryption and Hash Algorithm](https://csrc.nist.gov/CSRC/media/Projects/Lightweight-Cryptography/documents/round-1/spec-doc/ace-spec.pdf), 2019.

[5] C. Dobraunig _et al._, [ASCON](https://csrc.nist.gov/CSRC/media/Projects/lightweight-cryptography/documents/round-2/spec-doc-rnd2/ascon-spec-round2.pdf), 2019.

[6] D. Bellizia _et al._, [Spook: Sponge-Based Leakage-Resilient Authenticated Encryption with a Masked Tweakable Block Cipher](https://csrc.nist.gov/CSRC/media/Projects/Lightweight-Cryptography/documents/round-1/spec-doc/Spook-spec.pdf), 2019.

[7] S. Banik _et al._, [GIFT-COFB](https://csrc.nist.gov/CSRC/media/Projects/Lightweight-Cryptography/documents/round-1/spec-doc/GIFT-COFB-spec.pdf), 2019.

[8] D. J. Bernstein _et al._, [Gimli : A cross-platform permutation](https://eprint.iacr.org/2017/630.pdf), CHES, 2017.

[9] Z. Bao _et al._, [PHOTON-Beetle Authenticated Encryption and Hash Family](https://csrc.nist.gov/CSRC/media/Projects/Lightweight-Cryptography/documents/round-1/spec-doc/PHOTON-Beetle-spec.pdf), 2019.

[10] D. Goudarzi _et al._, [Pyjamask](https://csrc.nist.gov/CSRC/media/Projects/lightweight-cryptography/documents/round-2/spec-doc-rnd2/pyjamask-spec-round2.pdf), 2019.

[11] C. Beierle _et al._, [SKINNY-AES and SKINNY-Hash](https://csrc.nist.gov/CSRC/media/Projects/Lightweight-Cryptography/documents/round-1/spec-doc/SKINNY-spec.pdf), 2019.

[12] A. Bogdanov _et al._, [SPONGENT: A lightweight hash function](https://eprint.iacr.org/2011/697.pdf), CHES, 2011.

[13] T. Byene _et al._, [Elephant v1](https://csrc.nist.gov/CSRC/media/Projects/Lightweight-Cryptography/documents/round-1/spec-doc/elephant-spec.pdf), 2019.

[14] J. Daemen _et al._, [The Subterranean 2.0 cipher suite](https://cs.ru.nl/~joan/Subterranean/subterranean_ToSC_preprint.pdf), 2019.

[15] J. Daemen _et al._, [Xoodoo cookbook](https://eprint.iacr.org/2018/767.pdf), 2018.

[16] J. Daemen _et al._, [Xoodyak, a lightweight cryptographic scheme](https://pdfs.semanticscholar.org/7be0/c4cd872f3acb816b1737975f17d8ede942b0.pdf), 2019.

[17] A. Chakraborti _et al._, [Security Analysis of HYENA Authenticated Encryption Mode](https://csrc.nist.gov/CSRC/media/Events/lightweight-cryptography-workshop-2019/documents/papers/security-analysis-of-hyena-lwc2019.pdf), 2019.

[18] S. Banik _et al._, [SUNDAE-GIFT](https://csrc.nist.gov/CSRC/media/Projects/Lightweight-Cryptography/documents/round-1/spec-doc/SUNDAE-GIFT-spec.pdf), 2019.

[19] T. Iwata _et al._, [Romulus](https://csrc.nist.gov/CSRC/media/Projects/Lightweight-Cryptography/documents/round-1/spec-doc/Romulus-spec.pdf), 2019.

[20] B. Chakraborty, M. Nandi, [ORANGE](https://csrc.nist.gov/CSRC/media/Projects/Lightweight-Cryptography/documents/round-1/spec-doc/orange-spec.pdf), 2019.

[21] A. Adomnicai _et al._, [Fixslicing: A New GIFT Representation](https://eprint.iacr.org/2020/412.pdf), TCHES, 2020.

[22] P. Schwabe, K. Stoffelen, [All the AES You Need on Cortex-M3 and M4](https://repository.ubn.ru.nl/bitstream/handle/2066/178459/178459.pdf?sequence=1), SAC, 2016.
