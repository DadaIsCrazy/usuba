---
layout: post
title: Evaluation of Usuba generated ciphers on Intel CPUs
date: "2020-08-09 00:00:00"
description: 
lang: en
locale: en_US
author: Darius Mercadier
excerpt: 
comments: false
hidden: true
---

## Usuba vs Human

- Ascon. Ref: https://ascon.iaik.tugraz.at/ && https://github.com/ascon/ascon-c
  => Usuba much better because interleaving

- ACE. Ref from LWC submission: https://csrc.nist.gov/projects/lightweight-cryptography/round-2-candidates
  => Usuba much better because unrolling & scheduling (or, alternatively, interleaving is slightly worse, but no need to unroll => smaller code).

- Gimli. Ref from LWC.
  + Uses "generalized mslicing" 
    -> requires less parallelism
    -> less performances (requires more shuffles)
    -> but less register pressures -> can interleave -> better performances on AVX2


- Pyjamask/Xoodoo/Gift: ref from LWC is not optimized for x86. Let's see how Usuba does.

  + Pyjamask: Usuba much better. Mostly because inlining and
    unrolling. Able to interleave matrix multiplication calls. Less
    slow down because of data dependencies. 
    
  + Xoodoo: Usuba a bit better. Some small changes to the ref code
    make it about even (slightly better?) with Usuba. Still, Usuba
    automatically scales.
    
  + Gift: Usuba much better. Mostly because inlining expansive linear
    layer. (leads to a lot less operations because some parameters of
    that function were constants)
  

- Spook/Clyde. Ref from LWC.
  => Usuba much better because 3-interleaving.
  TODO: Shadow (ref implem is on SSE and AVX, cf https://git-crypto.elen.ucl.ac.be/spook/spook-he/-/tree/master/src)


- TEA?


- Chacha20/Serpent/AES/DES/Rectangle (PLDI)


## Scalability

The figure below shows the scaling of our implementations on the main
SIMD available on Intel: SSE (SSE4.2), AVX, AVX2 and AVX512. Those
benchmarks were compiled using Clang 9.0.1, and executed on a Intel(R)
Xeon(R) W-2155 CPU @ 3.30GHz running a Linux 5.4.0.

<p align="center" style="margin-top:30px;margin-bottom:30px;">
<img style="height:auto;width:auto;max-width:100%" src="{{ site.baseurl }}/assets/images/blog/eval-scaling-speedup.png">
</p>

We do not report results for all of the ciphers we implemented in
Usuba, but only for a few representative ciphers:

 - In bitslice mode, DES (a hardware-oriented cipher), AES (a more
   "mathematical" cipher), and Rectangle (which is very efficient in
   both software and hardware)
   
 - In hslice mode, Rectangle and AES, the main two ciphers that
   benefit from hslicing.
   
 - In vslice mode, Chacha20 (which relies on multiplications and
   additions), and Serpent and Rectangle (both of which are naturally
   vsliced).

We omitted the cost of transposition in this benchmark to focus solely
on the cryptographic primitives. The cost of transposition depends on
the data layout and the target instruction set. For example, the
transposition of u<sub>V</sub>16×4 costs 0.09 cycles/byte on AVX512
while the transposition of u<sub>H</sub>16×4 costs up to 2.36
cycles/byte on SSE.

One could expect the speedup to be linear in the size of registers,
but the reality is more complex. Spilling wider registers puts more
pressure on the L1 data-cache, leading to more frequent misses. To
stay within thermal design power (TDP) envelope of the CPU, wider SIMD
execution units (from AVX to AVX512) are throttled, meaning that the
processor must perform time-consuming frequency reduction steps. AVX
and AVX512 registers need tens of thousands warm-up cycles before
being used, since they are not powered when no instruction uses
them. SSE instructions take two operands and overwrite one to store
the result, while AVX offer 3-operand non destructive instructions,
thus reducing register spilling. Also, successive generations of SIMD
instructions expand the number of registers (from 8 with SSE, 16 with
AVX and 32 with AVX512), further reducing the need for spilling. The
latency and throughput of most instructions differ from one
micro-architecture to another and from one SIMD to another. For
instance, up to 4 general purpose bitwise operations can be computed
per cycle, and only 3 SSE/AVX. Finally, newer SIMD extensions tend to
have more powerful instructions than older ones. For instance, AVX512
offers, among many useful features, a bitwise ternary logic
instruction (`vpternlogq`) that computes any three-operand binary
function at once. We distinguish AVX from AVX2 because the latter
introduced shifts, _n_-bit integer arithmetic, and byte-shuffle on 256
bits, thus making it more suitable for slicing on 256 bits.

ONLY 2 AVX512 ALU !!!!!

Bitslice Rectangle and DES both scale nicely. As any bitsliced
algorithm, they both suffer from a lot of spilling, and therefore
benefit a lot from having more registers available: using AVX
instructions instead of SSE allows to use non-destructive 3-operand
instructions, therefore yielding a 10 to 15% speedup, without using
wider registers. When doubling the word size by using 256-bit AVX2
registers instead of 128-bit, the speedup is unsurprisingly
doubled. AVX512 benefits are twofold. First, they provide twice more
registers than AVX2, thus reducing the spilling. Second, they offer a
bitwise ternary logic instruction, which is exploited by Clang to fuse
nested logical gates. Overall, this allows Rectangle and DES to run
almost 5 times faster on AVX512 than on SSE.  Bitslice AES, on the
other hand, does not benefit from having wider registers, because it
contains many more spilling and memory operations: about 6 times more
than Rectangle and DES. Consequently, increasing the register size
puts more pressure on the caches: while AES suffers from less than 1%
misses in the L1 data cache when using general purpose 64-bit
registers, that number grows to 6% on SSE, 14% on AVX2, and up to 50%
on AVX512.

Because the frequency of the CPU is throttled when using AVX512, the
simple increase in register size when using AVX512 is not enough to
yield a x2 speedup when compared to AVX2. For instance, Serpent on
AVX512 contains 2.44 times less instructions than on AVX2, and yet
executes in only 1.7 times less cycles. The ciphers that reach 

LAZY LIFTING RECTANGLE

Chacha20 AVX512: 130 instrs; AVX2: 240 instrs (1 in 3 instruction is a
rotation -> represent 3/5 of all instrs in AVX2 -> only 1/3 in in
AVX512 (not exactly because half the rotations are done with
shuffles)). + more registers = no need to spill!

Overall, m-sliced programs are similar excepted when it comes to
shuffle and m-bit arithmetic. As a result, they exhibit similar
scaling behavior. They tend to use only a handful of variables, and do
not suffer from spilling as much as bitsliced implementations, which
explains why using AVX instructions yields only a marginal –if any–
improvement in throughput. As in bitslicing, AVX2 registers allows the
throughput to be doubled. AVX512 is particularly beneficial because of
the new instructions it offers: the aforementioned `vpternlogq`   instruction. This translates into a good
speedup on Rectangle and AES while the effect is reduced on Chacha20,
which involves few Boolean operations.

Since CPUs with AVX512 offer more registers than CPUs with only AVX2
or SSE, Usubac can interleave more aggressively without introducing
too much spilling. For instance, our hsclied AES is 1.15 times faster
when interleaved on our Xeon W-2155, reaching 1.13 cycles/bytes
(excluding transposition), whereas on a i5-6500 (which does not have
AVX512 extensions), interleaving AES results in at x1.10 slowdown.


## Monomorphization

The relative performances of hslicing, vslicing and bitslicing vary
from a cipher to another, and from an architecture to another. For
instance, on Pyjamask, bitslicing is about 2.3 times faster than
vslicing on SSE registers, 1.5 times faster on AVX, and as efficient
on AVX512. On Serpent however, bitslicing is 1.7 times slower than
vslicing on SSE, 2.8 times slower on AVX, and up to 4 times slower on
AVX512. 

As explained in the [post describing the Usuba language]({{
site.baseurl }}{% post_url 2020-02-24-usuba %}), we can specialize our
polymorphic implementations to different slicing types and SIMD
instruction sets. Usuba thus allows to easily compare the performances
of the slicing modes of any cipher. In practice, few ciphers can be
not only bitsliced but also vsliced and hsliced: hslicing (except on
AVX512) requires the cipher to rely on 16-bit (or smaller) values,
arithmetic operations constraint ciphers to be only vsliced,
bit-permutations (declared with `perm` in Usuba) prevent efficient
vslicing, and often hslicing as well, _etc_.

One of the few ciphers compatible with all slicing types and all
instruction sets (except hslicing on general purpose registers) is
Rectangle. Its type in Usuba is:

```lustre
node Rectangle(plain:u16x4, key:u16x4) returns (cipher:u16x4)
```

Compiling Rectangle for AVX2 in vsliced mode would produce a C
implementation whose type would be:

```c
void Rectangle (__m256i plain[4], __m256i key[26][4], __m256i cipher[4])
```

and which would compute 256/16=16 instances of Rectangle at once.
Targetting instead SSE registers in bitsliced mode would produce a C
implementation whose type would be:


```c
void Rectangle (__m128i plain[64], __m128i key[26][64], __m128i cipher[64])
```

and which would compute 128 (the size of the registers) instances of
Rectangle at once.


We leave for future work to modify Usuba's auto-tuner to be
able to compare all slicing types for the desired architecture. 


In the following, we analyze the performances of vsliced, hsliced and
bitsliced Rectangle on general purpose registers, SSE, AVX, AVX2 and
AVX512; all of which were automatically generated from a single Usuba
source of 31 lines. We ran this comparison on a Intel(R) Xeon(R)
W-2155 CPU @ 3.30GHz, running Linux 5.4, and compiled the C codes with
Clang 9.0.1.


<p align="center" style="margin-top:30px;margin-bottom:30px;">
<img style="height:auto;width:auto;max-width:100%" src="{{ site.baseurl }}/assets/images/blog/slicing-compare.png">
</p>

REDO GRAPH WITH AVX512 FIXED (alignment issue)!!!


Overall, mslicing is more efficient that bislicing on Rectangle. The
main reason is that it uses much less registers and thus do not
require any spilling. Hslicing and vslicing do use more instructions
per bit encrypted, since they perform the rotations of Rectangle's
linear layer at run-time (whereas they are computed at compile time by
Usubac in bitslicing), but this small extra cost does not offset the
improvement offered by their low register pressure.

When excluding the transposition, hslicing is faster than
vslicing. The main reason is that vsliced Rectangle requires 3
operations per rotation (since SSE and AVX do not offer rotate
instructions), whereas hsliced Rectangle only need one `shuffle` per
rotation. Without interleaving, both vsliced and hscliced Rectangle
would have similar performances however, because Skylake can only
perform one `shuffle` per round, and up to 3 shifts per round: the
`shuffle`s would need to be computed sequentially, thus not fully
exploiting the superscalar pipeline. However, thanks to interleaving,
while the `shuffle`s of one encryption are being computed
sequentially, the S-box of another (interleaved) encryption can be
computed at the same time (since it uses other ports of the CPUs).

On 64-bit general purpose registers, bitslicing is actually faster
than vslicing because the latter processes only one block at a time,
as a consequence of the lack of x86-64 instruction to shift 4 16-bit
words in a single 64-bit register. Still, the `_pdep_u64` intrinsic
could be used to interleave several inputs of vsliced Rectangle in the
same 64-bit register. This instructions take a register `a` and an
integer `mask` as parameters, and dispatches the bits of `a` to a new
registers following the pattern in `mask`. For instance, using
`_pdep_u64` with the mask `0x8888888888888888` would dispatch a 16-bit
input of Rectangle into a 64-bit register as follows:

<p align="center" style="margin-top:30px;margin-bottom:30px;">
<img style="height:auto;width:auto;max-width:100%" src="{{ site.baseurl }}/assets/images/blog/pdep_u64-1.png">
</p>

A second input of Rectangle could be dispatched into the next bits
using the mask `0x4444444444444444`:

<p align="center" style="margin-top:30px;margin-bottom:30px;">
<img style="height:auto;width:auto;max-width:100%" src="{{ site.baseurl }}/assets/images/blog/pdep_u64-2.png">
</p>

Repeating the process with two more inputs and the masks
`0x2222222222222222` and `0x1111111111111111`, and combining the
results (using a simple `or`) would produce a 64-bit register
containing 4 interleaved 16-bit inputs of Rectangle. Since Rectangle's
whole input is 64-bit, this process needs to be repeated 4
times. Then, regular shift and rotate instructions can be used to
independently rotate each input. For instance, a left-rotation by 2
can now be done by a simple left-rotation by 8:

<p align="center" style="margin-top:30px;margin-bottom:30px;">
<img style="height:auto;width:auto;max-width:100%" src="{{ site.baseurl }}/assets/images/blog/rectangle-std-inter.png">
</p>

In essence, this technique is similar to hslicing in its data layout,
but similar to vslicing in the instructions it offers. Bits of the
input are split along the horizontal direction of the registers, but
the lack of vector instructions on general purpose registers prevents
the use of `Shuffle`s, and requires using standard vsliced
instructions, except for arithmetic instructions. We leave for future
work the investigation of how this mode of slicing could be
incorporated within Usuba.




## Transposition ?

(from WPMVP)


---
## References
