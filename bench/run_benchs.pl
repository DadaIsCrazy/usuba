#!/usr/bin/perl

use strict;
use warnings;

use FindBin;
use v5.14;


=head1 USAGE
    
./compile.pl [-g] [-c] [-r]

    -g: Re-compiles the Usuba sources to generate the C files (optional; not default)
    -c: Re-compiles the benchmarks (needed at least the first time; default)
    -r: Run the benchmarks (default)
    (the options can be combined)

=cut





=head1 BENCHMARKS IN SECTION 3.2

All the results presented in Section 3.2 ("Back-end") follow a common
pattern that we describe here, once and for all.

=over

=item

B<Core technical details>

    The actual measurements are done by the files main_speed.c. They
    use
    [_rdtsc](https://software.intel.com/sites/landingpage/IntrinsicsGuide/#text=rdtsc&expand=4512)
    to accurately measure cycles. They run NB_LOOP (default: 100000)
    encryptions of a BUFF_SIZE bytes (default: 4096B) buffer.

    Each benchmark produces a .tex file that contains macros defining
    the numbers reported in the paper. In the following, we relate
    these macros to their use-site in the paper.  The raw values are
    to be found in the "results" subfolder.

    Note that when code size decreases thanks to an optimization
    (instead of increasing such as, e.g., when inlining DES), the
    generated macro contains a negative number. 

=item

B<Customization>

    The benchmarks are ran $NB_LOOP times (defined at the beginning of
    run.pl, default to 20). A single benchmark takes about 5 minutes
    to run.

    The script has 3 parts, configurable through command line flags:
     * -g: regenerate the C codes from the Usuba sources. (not by default)
     * -c: recompile the C binaries (default).
     * -r: run the benchmark (default).

=back

=cut
    






=head2 INTERLEAVING

=over

=item 

B<Paper>: Section 3.2 ("Back-end"), paragraph "Interleaving"

=item 

B<benchmark directory>: bench/interleaving

=item 

B<benchmark command>: bench/interleaving/run.pl.  


=item 

B<High-level description>

    This benchmark runs interleaved and non-interleaved versions of
    Serpent and Rectangle, as generated by Usuba, and produces the
    file results/interleaving.tex that contains the speedup and binary
    size increase due to interleaving.  It will also generate
    results/{serpent.txt,rectangle.txt} that contain the details of
    each measurement (those numbers are also be printed on stdout).

=item

B<Specific details>

    The C code generated by Usuba are already present in the directory
    (named xxx_ua.c). They can however be recompiled; refer to the -g
    flag above.  The other C files are part of the cryptographic
    runtime, not generated by Usuba (stream.c, *.h, serpent.c, key.c, 
    etc.)

=back

=cut

say "Running interleaving benchmark";
chdir "$FindBin::Bin";
chdir "interleaving";
system "./run.pl @ARGV";








=head2 SCHEDULING

=over

=item 

B<Paper>: Section 3.2 ("Back-end"), paragraph "Scheduling bitsliced code"

=item 

B<benchmark directory>: bench/scheduling-bs

=item 

B<benchmark command>: bench/scheduling-bs/run.pl.  


=item 

B<High-level description>

    This benchmark compares the codes produced by Usuba for bitsliced DES
    and AES, with and without scheduling.
    It generates the file results/scheduling-bs.tex, which contains
    the macros \SchedulingBitsliceXYZSpeedup (speedup gained by
    scheduling on cipher XYZ) and \SchedulingBitsliceXYZCode (code
    size increase/decreased due to scheduling on cipher XYZ). Those
    numbers come in the paper right after "On bitsliced DES, scheduling
    after inlining increases throughput by ...".

    
=item

B<Specific details>

    High-level view of run.pl: 
    lines 57-58: generate C code from Usuba
    code without (-no-sched) and with (no options; scheduling is done
    by default) scheduling.
    lines 68-69: compile the binaries for the benchmark (with and without 
    scheduling)
    lines 85-89: run the benchmarks, store the results.
    lines 95-104: print measurements to stdout (and to the .txt files)
    lines 106-113: compute the speedup/sizes.
    lines 117-end: print the results.

=back

=cut

say "Running bitsliced scheduling benchmark";
chdir "$FindBin::Bin";
chdir "scheduling-bs";
system "./run.pl @ARGV";







=head2 INLINING

=over

=item 

B<Paper>: Section 3.2 ("Back-end"), paragraphs "Inlining", "Scheduling bitsliced code" and "Scheduling m-sliced code"

=item 

B<benchmark directory>: bench/inlining


=item 

B<benchmark command>: bench/inlining/run.pl.  


=item 

B<High-level description>

    This benchmark generates the file results/inlining-nosched.tex and
    results/inlining-sched.tex, which contain the following macros:
    * inlining-nosched.tex: (bitsliced ciphers only, no inlining)
      - \InliningNoschedXYZSpeedup: the speedup offered by inlining in cipher XYZ
      - \InliningNoschedXYZCode: the increase/decrease in code size caused by inlining
    * inlining-sched.tex:
      This might be confusing, but the meaning of "scheduling" in this
      file depends on the slicing type: for AES and DES (bitsliced),
      it is bitslice scheduling, whereas for Chacha20 and AES
      H-sliced, it is m-slice scheduling.
      - \InliningSchedulingXYZSpeedup: the speedup offered by inlining and scheduling
      - \InliningSchedulingXYZCode: the increase/decrease in code size caused by inlining

    More specifically, the numbers in the paragraph "Inlining" are
    \InliningNoschedDESSpeedup (44.8), \InliningNoschedDESCode (9.1),
    \InliningNoschedAESSpeedup (24.24), and \InliningNoschedAESCode
    (24.8) (in that order). 
    Paragraph "Scheduling bitsliced code": only the last two numbers
    ("Overall, combining inlining and scheduling results in a net..")
    are generated by this benchmarks (the previous four number are
    generated by bench/scheduling/run.pl):
    \InliningSchedulingDESSpeedup (45.8) \InliningSchedulingAESSpeedup
    (26.22).
    Paragraph "Scheduling m-sliced code": only the first two numbers are
    generated by this benchmark ("This scheduling algorithm increased the
    throughput of ..."): \InliningSchedulingHAESSpeedup (2.43) and
    \InliningSchedulingChachaSpeedup (9.09).

    The details of each measurement are available in results/*.txt,
    and are printed to stdout during the benchmark, though you shouldn't
    need them.

=item

B<Specific details>

    High-level view of run.pl: 

    lines 32-44: ciphers for the benchmark, and their slicing types.

    C file generation:
    line 61: $sched_opt will be either -no-sched (ie, don't perform scheduling),
    or '-sched-n 10' (ie, perform scheduling, and sets the look-ahead window to 
    10 for the m-slice scheduling).
    line 65-66: compiles the Usuba programs, either without inlining (-no-inline),
    or with inlining (-inline-all).

    C file compilation:
    lines 75-80: compile each cipher for each combination of scheduling/inlining

    Benchmark run:
    line 95-98: run each binary for each cipher (l.88, for my $cipher),  each 
    scheduling (l.87, for my $sched), and each inlining (l.94, for my $inline),
    and store the results inside the hash %res.
    line 104-113: generate the .txt files (which contains the details of the
    measurements).
    line 115-122: compute the speedups, and code size ratios.
    line 126-end: generate the .tex files, containing the numbers computed above.

=back

=cut

    
say "Running inlining benchmark";
chdir "$FindBin::Bin";
chdir "inlining";
system "./run.pl @ARGV";






=head2 UNROLLING

=over

=item 

B<Paper>: Section 3.2 ("Back-end"), paragraph "Scheduling m-sliced code"

=item 

B<benchmark directory>: bench/unrolling

=item 

B<benchmark command>: bench/unrolling/run.pl.  


=item 

B<High-level description>

    This benchmark compares the effect of unrolling on the codes
    generated by Usuba for m-sliced AES and Chacha20.

    It generates the file results/unrolling.tex, which contains the
    macros \UnrollingXYZSpeedup (speedup gained by unrolling cipher
    XYZ) and \UnrollingXYZCode (code size increase/decreased due to
    unrolling cipher XYZ). Those numbers come in the paper at the end
    of the paragraph "Scheduling m-sliced code", right after "On AES
    (resp. Chacha20), this yields a ...".

    
=item

B<Specific details>

    High-level view of run.pl: 
    lines 60-61: generate C code from Usuba code without (no options;
    unroll is not done by default) and with (-unroll) unrolling.
    Note that scheduling and inlining are enabled (line 56).
    lines 71-72: compile the binaries for the benchmark (with and without 
    unrolling)
    lines 88-94: run the benchmarks, store the results.
    lines 98-107: print measurements to stdout (and to the .txt files)
    lines 109-116: compute the speedup/sizes.
    lines 119-end: print the results.

=back

=cut
    
say "Running unrolling benchmark";
chdir "$FindBin::Bin";
chdir "unrolling";
system "./run.pl @ARGV";






=head1 TABLE 3

=over

=item

B<Paper>: Table 3. "Comparison between Usuba code & reference implementations"

=item 

B<benchmark directory>: bench/ua-vs-human

=item 

B<benchmark command>: bench/ua-vs-human/run.pl.

=item 

B<High-level description>

    This benchmark generates the file human.tex, which defines the macros with
    the throughput, code sizes and speedup displayed in Table 3.

    It uses three different sub-benchmarks the run the main
    benchmarks, and only gathers the results afterward.

B<sub-benchmarks>: 

    1. The benchmark for DES can be found in des/compile.pl and
    it very similar to the scripts running the benchmarks for section
    3.2 (see above).

    2. The benchmark for Rectangle is ran by
    rectangle/rectangle_ua/run.pl for the Usuba code, and
    rectangle/rectangle_ref/run.pl for the reference code. The
    structure of the latter is slightly different from the other
    benchmarks, because it relies on C++ code (the code provided by
    the authors of Rectangle), but works in a similar way. 

    3. The benchmarks for AES (H-sliced), Serpent (V-sliced) and
    Chacha20 (V-sliced) rely on Supercop. Be advised that running
    Supercop will take a while (around a few hours). Supercop produces
    files containing the number of cycles needed to encrypt a buffer
    of a given size (see for instance
    '/supercop-data/dadaubuntu/amd64/try/c/clang_-march=native_-O3_-fomit-frame-pointer_-fwrapv_-std=gnu11/crypto_stream/chacha20/usuba-avx-fast/data').
    This benchmark is _not_ ran by this script. To run it, you must
    delete the folder supercop-data, go in the folder supercop and run
    './data-do'. This _will_ take a few hours. We added in the
    repository the resulting files relevant for us that were produced
    by a `./data-do` (inside supercop-data).


B<Collecting the results>. (this is actually the content of bench/ua-vs-human/run.pl).


    1. DES (lines 47-75): the results are already nicely formatted in
    des/results.txt, which was generated by des/compile.pl. 
    To the code size of Usuba, we add the size of the circuits for the
    S-boxes, which can be found in 'data/sboxes/des_*.ua' (and amounts to
    about 500 lines).

    2. Rectangle (lines 308-360): The results are in
    rectangle/rectangle_ua/results.txt and (generated by
    rectangle/rectangle_ua/bench.pl) and
    rectangle/rectangle_ref/results.txt (generated by
    rectangle/rectangle_ref/bench.pl). Both of them are already nicely
    formatted; nothing much going on here.

    3. Supercop (lines 77 to 300). For each cipher and architecture,
    the variables $ref_file and $ua_file contain the filename of the
    best result for both Usuba and the reference implementation. The
    "interesting" bits of the filename are:
      - crypto_stream/.... : shows which cipher/implementation is being considered
      - try/c/....-fomit : shows which compiler and compiler flags are being used.
    The function get_speed_supercop (lines 364-379) finds the lines
    containing "xor_cycles 4096" in the data file (which means that a
    buffer of 4096 bytes was encrypted), and computes the average of
    the numbers that follow on those lines, eliminating the lower
    values (which almost every time are to blame on context-switch or
    other OS-related issues).
    Counting code size, it has not been automated globally because
    reference implementations tend to fuse runtime and primitive, or
    include unrelated functions (such as the decryption function) and
    thus a simple `cloc` would not provide a fair comparison. We
    manually counted the number of lines of code and hard-coded the
    numbers, with some details when possible (see line 88-92 for
    instance). Both the Usuba and reference source codes can be found
    in supercop/crypto_stream/xxx.

=back


=cut

say "Running ua-vs-human benchmark";
chdir "$FindBin::Bin";
chdir "ua-vs-human";
system "./run.pl @ARGV";










=head1 FIGURE 3

=over

=item

B<Paper>: Figure 3. "Scalability of SIMD compilation"

=item 

B<benchmark directory>: bench/scaling-avx512

=item 

B<benchmark command>: bench/scaling-avx512/run.pl

=item 

B<High-level description>

    This benchmark generates the file plot/speedup.pdf, which is the
    Figure 3 in the paper.
    Raw data can be found in plot/data-speedup.dat.

    If you just want to generate speedup.pdf from pre-computed
    measurements, you can simply run `./scaling-avx512/run.pl -l`;
    this should be really fast.


B<Technical details>

    Each folder (except 'plot') within bench/scaling-avx512 contains a
    generic (in the sense that it supports various architectures)
    runtime for a given cipher.  A script 'compile.pl' in each
    directory takes care of generating the C files, compiling them and
    running the benchmark for the cipher. If your computer doesn't
    have AVX512 registers available, we simply report '0.00' instead
    of the AVX512 measurements. The architecture of these scripts is
    similar to those presented in Section 3.2.

    The script scaling-avx512/run.pl takes care of calling each of uts
    compile.pl scripts, and then gathers the results, normalizes them
    (in order to always have "speed SSE = 1" followed by the other
    speed-ups relatively to SSE), and then generates Figure 3.


=back

=cut

say "Running scaling benchmark";
chdir "$FindBin::Bin";
chdir "scaling-avx512";
system "./run.pl @ARGV";








=head1 FIGURE 4

=over

=item

B<Paper>: Figure 4. "Monomorphizations of Rectangle"

=item 

B<benchmark directory>: bench/rectangle

=item 

B<benchmark command>: bench/run.pl

=item 

B<High-level description>

    This benchmark generates the file plot/slicing-compare.pdf, which is
    Figure 4 in the paper.
    Raw data can be found in plot/*.dat, and plot/results.txt.


B<Technical details>

    This benchmark is similar to the ones presented in Section 3.2.
    It first compiles the Usuba files (if -g option was supplied) with
    each slicing mode (-B for bitslice, -V for vslice, -H for
    hslice). It compiles the C benchmarks and run them.
    The actual measurements are performed in 'main.c'.


=back

=cut

say "Running rectangle/monomorphization benchmark";
chdir "$FindBin::Bin";
chdir "rectangle";
system "./run.pl @ARGV";
